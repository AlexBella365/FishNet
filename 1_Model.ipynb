{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT = os.path.join(os.getcwd(),'img')\n",
    "img_folders = {\n",
    "    'clown':os.path.join(ROOT,'clown'),\n",
    "    'reef':os.path.join(ROOT,'reef')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RESULT = {\n",
    "    0:'Clown Fish',\n",
    "    1:'No clown fish'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in one loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    target = torch.LongTensor(target)\n",
    "    return [data, target]\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.Resize(size=64),\n",
    "                                       transforms.CenterCrop(size=64),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ])\n",
    "\n",
    "# change root to valid dir in your system, see ImageFolder documentation for more info\n",
    "dataset = datasets.ImageFolder(root=ROOT,\n",
    "                                    transform=train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = round(len(dataset)*0.7)\n",
    "indices = list(range(len(dataset)))\n",
    "train_ids = list(np.random.choice(indices,size=N,replace=False))\n",
    "test_ids = list(set(indices) - set(train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "train_sampler = SubsetRandomSampler(train_ids)\n",
    "test_sampler = SubsetRandomSampler(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = DataLoader(dataset=dataset,\n",
    "                      batch_size=8,\n",
    "                      #shuffle=True,\n",
    "                      sampler = train_sampler\n",
    "                      #collate_fn=my_collate # use custom collate function here\n",
    "                      )\n",
    "testset = DataLoader(dataset=dataset,\n",
    "                      batch_size=4,\n",
    "                        #shuffle=True,\n",
    "                      sampler = test_sampler\n",
    "                      )\n",
    "\n",
    "trainiter = iter(trainset)\n",
    "imgs, labels = trainiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clown Fish\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAg3ElEQVR4nDW6ybOmyXXe95yTw/u+\n33iHukNVV3VVd1dXVwM9EU2gQYxSiBQlO8xg0GFFSA4pHEHvvPFCWmml8F9gh730SiHJtkIDCQ+S\nSYtAEwAhoAk0CPSAnqprvHXr1p2+4R0y85zjxYU2uc4TJ/Pk83uepD/8X64hiy1Kfd7NOdcT1Sm6\nfV5tb5etF0i3h+8f2t0+gBtqvTsvk0qmlbjRdGu/4s3hyR2R5JuN7hgH/+dfqWgM3EzmHz2Vxyft\ns9euvHHrRS6DaibnvPO561LfpX4N6epRXU02na8JzMxxPK6m02e+sdz5xm+q7Ajlx++9f/+fv6O9\nY9CVN5Yv/d2vV/Gawg+lfPLvnh795MRfy37yVC2PMDToadVRvxI+eZiXK8tnyAcyr+IbV7Q9wkE3\n2KY1G67A1qZVr0NPobCQGUHIhYpDDQCmB+f6+YOzZ2o+vnPvQ8kvPHslxpqoIvZSVgQZjedmMx8r\n5yNBmZhAkospYbChO+5Vlbi5Oo5bzfC4wGj5EO0nb7tn95XG68eb3cNxmM9IzKfH8zhcNa1UzIoN\nrXJsnPzKU0/Dk9x6mVn4wsSenOvZIM2Mubk8fpYbd5I/BykJyMBmLlS+GguEfH14cPrKBF/eKOcD\n//jho1Wb3nz1C1XVOOdBpCreaTOa1ZNtkE/DCUOJiBSa5OzTFG7+dJjfztYYCc8qOhACVk+rD//N\nk0s3f+His6eHs+7pKk7nVrynl6+VA5N7j3WVVEyNCS4O28EeEw2gE8sujy/7Lz2D792lUtZn6f2P\nf/XNN7802fvi4fAYFkiLZam39ybXUnv/Xo6jeXn41y/JzOvcq4aq8LLRAvLO18SBQWQGzZrXHEae\nA5MBAJFJXh9t+B/fsa92A65pCLQVXTAt2cSdPZ4tn26Eya4fBdWcu84keWmf0qUKTdTP+3ywlEGI\nXMm+5rGr1jzqxU5s8DJ7ht+6nj85/fF3P/70w7Pv/flHv/O337r26iUIAwxRH3jnlVePBj0/PX5t\nrJdHwgrv7dUmuUD3YNmIQh3qmbWnQDHNWnoi874iJjMjMjLA6pMP9uqd+6sdLRSbnUvVbGtYtqJC\n6KGkaaBG4mgqQ6/diu10IQeHuj52V2P10iV/aUKOpeT+pMHT2j0Wtx5oWOLshHeqcnXr6Lgtoqen\n3b//4//45IPzGGYwYoGl3o1s9/bLyzaNnHjSxuvY6YTL2JVgAlEm10y3XDUiYoKSFhMhIsfBueBc\ndOTYOcil/NG0rBbp9EmanMftcTPeZq6ZA5GHmqaOiZyv2NUefUKfMIiEJUaz6vq2XY758Gx4crp6\nHMZefcWyNags7CQibnDwOwFvTmke0+HP3t+7/m22hZmiKOe1ssWSxuSgGkMhsBgYYCYCoBbGs3q6\ng9wRFOzY1xxq5wNM4WCB0JivJ7q67Lung54POLJnbtTtVuqWYj1RYQKkWBrIIpl57gbtsiyLaqa6\nYJzc9FL93BW/e3l99+H66PE0DgyXNzuVswB/6VLz0nr19Uu5ruyz/PjksNt45mru71ORYDTbHF2Z\n8aYKkVSenKqClIxgZGJ58M0oTra5pDiasYNjkPMg1bGUuWa3Fj2H9d7ByW5Vbcpwut44nIboPCHO\nSDvHjqEk2TmnKL6cDkyk2TQVKoCsKZn2XZzs17dv9ceXdbgTThfiRUctucW1a83eIYEMRvuuvPfk\nye2v/ZdPPn/bDW7eTEabWxVsEig6cgRmEjUNntgxDJK45PF817KMZ9vOU+7PpSx5t26n7dCd6fLM\n8ootWzBKHDa33fRG8kO/fU5r9dRQBmAEYs1ggyZ/+liaxnvyzIZCaEVyz4OWdeFmVY8u+fiG9Z/4\no8eyITbtLt2o+D2fRI0QHEbtYu+ZG+Pg8up4b39vIZr6VDmMvQssqiLkhuLNRRBDVfLQTGbNpauh\n2rIyqKA0Q5r10p7Res19bzmxJSoGdWQrXZ5SsNUo8q0qlJFbhHAO7kGmJj2heEm0TmBFM/JVTWaw\nJGXodJ04DhLXOt6McdPlE6hAdNR426q7J60jgFAZN+ONrdvbVNazyXjxwUeh76uRj1S8CZiIkMk1\n852UmIjIzKRUtZ9c2hQwj6pclv3igNtzkkFJ1BGpI2eAWVpyzkAhM2Hqg5dZ4O1JsHHIhTulpXop\nYIKJta0VoWbkmVRENIulLNxpu7Z67sOEyxlpoY0Ke+Pu0Wrs2JhmwW+N6tl0BmxFxtn9J5dpmIRU\ne6pAADlw64MfbdZ1yP2aOZBa7tbI62q6I2WS753o0YENSxetGnmu5lxNLZbMq2wLBylqMGExKwkD\ngxd94HVgGxPmzpsyBx8cO08wHTrxntl7QFWUtIiuUh7ExdAwlYGo4o15iidFdUI8HY3HoWp8JFA/\n9Kd/9bPbozKNVJF6KGDR+LBv3/n5z179yre895YTgbQf0rp1ddGchsdHw3LluDgOlJ13V5vRy/V8\ngyrrh4dt9/kqf5jdeWEVE5jBwDpQgiVTUj/d2GICiQCmklU0iXl13gdyBVqgqpqlH6SneurIOprM\ndGNqZ8tJU9XPv1Q3E8AB+PzOvY3P3tkbsfNjmAKDQwJsy+m9D9+n6eabr305dy3ARgwllWxmmo2M\niYJkk9w7p2SRtamr+bh5duRfoQfVevn/xJGVJkqI6lXhFIAVRvExjlXVqMAAeKYMNckws1AxmTM1\nMzNRK9pBanNsrc2aydCH+WT+pW8IsSkIePCj797Esq4mcJ5UDKFIx5TnjX1rS//43XeuP3t9d2uP\nROtm7sczKarFCJ6NUKBFwQRL9XjKjkq7AGnpO+f2ZTW184cU3cbe61ptDHYkri/cGpsnYmZSIjIQ\nGURBZKbMRUomgyNnMBBgkEEH9A5c+bg1Ydu5sfX8rSIIoJOjo+rd787rSN4DBnZEDIJqMsrfvuzf\nW7Uf/eqD63/zVuCmnm2H8VSMXBbnx2IeBCJPIMsd2Qol5z7l1Jc0oBTnd9NwYKUM58tq86YvNdnK\n+UF55YECGJuqlmEYTAoTxwpERVKBmaJ4H0CkIFOyTlppZ1UZeRpufKEez0pWIpze+3h7dbeuYHAw\ngKDwBgcGI8+i/d3r9b9bHjgn88v7o8099k0uyYdQT3fy8g4TgQBCGfpheVhP90jNqZiJmMS4P6ze\nM5NhcT+M90ARMDgn87HX0olIzjkNKWcxctUkVjWsAAzLZEQiMCN2gUlMJQ86KR0aF557yUGgIA6r\no8Nt6YI5IQ+CAcWgMDKGaVbabeR3R2ero3s627PJnFzwBKvceLrT+oYvWgwzsdwtQpwTGXQw7VgH\n7yrmaclDSW3pP/OzSzKh/hleNa1fLs7U1MypMTjEjWa2WznNNhRyXpOWlNtBidkzog/svC9lmxJF\nN720bypEDCvp7NhRUTXmYhTNYMiqOQspGGA1nutq9OA/rrdvrH0Tp0IuiuRQjUKcshhIzcyISlrL\nsCQiya2kTq13VRs35+XkBJA2ntK1adeYtAt3fObNHIxhMLN6azS9scVI1DNc8DWVNuUs4xuz/dee\nq6t49unB8rPjqWHmko1HYb6hpo5UpeSnB04HEmOoIJKBNeWSe+VkAYDB94Wqg4/rnV+Ujat5GIhL\nzolZm9leaRk2mA4KMikytOw8AbFeN1c/rm763l375O3FEKnbbNCe82nvOJkXzxxVBaSjnfnk5o53\nZsmRA5htPaST1Y3ffumb//Xf2b980zOtFw/f+df/1/pf/2RCIps7YWPj1yIT5h2TZRUly4SgRqa0\nLvW9YfJQJhKa2IwXKcfl+uWPfzp6/i0bz5jZO/YhVM2UNZkVLa1IInLELowaT73jd9l+Tv3NsnNN\nn79WHh/4JyugIBiDq8WGJwJ5N3pmd3LrCpCQMleBJgHr4ezwcHJ18vX/5vcub91sbBpBo40rX/2D\nr919/5fjz5+2+8/Wo4aYYQDIszDIzMzUjMXoYBj/srrNX/r6F7/yjf2rVzZnk9WQHj16cueH399+\n/NnG3k32Dlqc9xxqjg20EDOZxtGkmW7F2vfrv+xXv5AcFy6cffqBnpwEEXhiItdZqDaq5qZ3lR/d\n2B+/cIUlaTIEJuepy+efHKSz7urvXpttTCHFqAU7xnJjHvn1fTy61+9cZsdQwBRgHjoHAGSGbHSQ\nJu9d/+tv/bf//auvvTqqKwaBYITbr7y4/K033/3zH+W+d1RZygywjy6OTDMJReea2bb30exxJ+92\nsVnOX+ifGLXHzgo59rnBulDbYipp68zPvnwr7mzQkEjVPLNjLNrT9+6vT3sieFlQOoLfAFm21soh\n50NnnZKNNjZMi+kF0IqWBAAgNm5L9emNb/7uP/ofXrh+nfhi8wBAQADPx/WXv/WVH/6HX7z7zg+7\n9XK9OJ41bn9n14fKHGIdXOWVczv6aFm5dXpJTnMYVkrmLIxlfyQ3+9yvhg8kr3g48fX+mPqWFXBw\nnmmxWn52xIkm01E9q7WTfPbTtMGgxvJTJ/fd4b3Tn35WFRLykrMZ2ABH5FmJBQDo3ujql/7Bf/fC\ns9cdO2IwwBcVAGYg0LSpx1P3L/63f35+elhERpV75srea7eff/Xl2xaiBsmXzp66s+HJVTtZ+pKM\nOEiY4nqlN008Uzuqb3X9Z+Vs6bnrQcwxEno+WbWfH2sr9Wh69WvXX//PvrJ1eacaSymn/fI9nN/T\no8Onv+Dl57rPJP0Q08BkBnLsYqzLhfakSfPbf+8Lb/ymIzBwYTiY4aILBoBA5l57/fZvvvmlH/zZ\nd7oiQ5c//OT+vfsPiwxvfO03Sj0s+NFw6uhw6XJRcq64qe03/HzXFpUekio3dc3z7fqhF5+rOKug\n+fFJe/csdeyrne0v7H/17399d+f5hkdilsL5ql8efPrR8Qez9QO/tT1vn34g5+dNvwxh7NlbURBA\nJrDh5pu/8Xt/r3KR6AKEAbvYPxGMCDAo0XQ++od/+Ld/p//OZ09SIf3RI3dnSX/yvZ+MnqfrO8+l\n4zU/XvicjIiEqv5S5D14k9QPqauDhxmbq+KOD9OqksEOV+W85slz9ahxbnLjy7NmVlnJhZKCmKP5\n3eN27/FTnB8utZrFTdKTs75dYORcaJgxijCCcrX1lb81n20xGUFFbLlcnR4+Xh48Gs6OdXlGDhSq\nEpvm0s716y/89te+qJ+8nVh++6p79zj8Tz/jn3//o73tbRse+dwrjMyN+bkq7pKoSfFsfcni2VFU\nU7PgZ01lZyE1237Ph1zZ4FyQemOd80lXXEbDPnT94jv/9O0/+z9+sT7SUrBdhTduvvjibthYtxIb\nCxHGuj4jME33Jy99GSaqpmcnH7/9p/d/8P9unXy4JYd73DfBnKch6TLR0iaHz76+sz2po4VKX/F2\neWoPF/yTjvP5gacEBhlm1c35+CvSD5IWeVgZUgjejPskIBZhb+FyroqWwq7iOBbrSM8Wh2VxeJCm\nax8mZ3fX//v//PaP/uTTblEi8Ze2eOb654Z367XK6hTTKQNQ42HhHMc3vj2+fANqi3e+133nf9x8\n8tPnaDn2JYyNIsjBGBC6nE3Xy/W9g/N7/ilsuu+nle44+/2XdJCJpyVBTcmXkfdbuQzsYwyXfNVk\nAaQ4DiKimkHm190aKYN95cd2erx88NDyUjMqdBvXVwj8nX/6wXf/6DPNeHVq39jLf+2qXN2QaWXq\n/+L+w2fanauOGKY8rMvm1Ut/4x+wC+uT41/9s//10uEPb2wPkw1hZY5m0YwJBohBjB0mRCXpZyex\ndX52uatJt+f1c03o1UBACkF3BluW9Kmr5vVoNzbzuaurZl5SW9KQh1XJyZslqnwcfDq4d/bx3W6x\nJkBS9JDl4fqv3jv+wf/3YOjTbvT/1XX9L14YYmXszSUGDfsPf/Do1rd4+3JeLYeT4+bl3xzvP2/Q\nxYP7P//0yc1MV7fBW0SqIDNnILCSqUFAjlDQd/QfHvBViS/sDGa4E/aTZ1KDuGB7jJnmpHk1tKt+\ncRSbjVDNzKCiF8pU89rHJtBp3338+ckn93NPasSMvtXDu+nBA3z3h8frFV2uq/98p3xzZ4jBfAUl\nlKwMjFb39h78qL16uz0/y4vTeOVlI98N3V/+xTv/7INHX63dF5/D5gahGCkIDAOUyGAFgNqITg7x\np4/xTNHffyEehtFHfgdmIsR57MImcSRzOXUynGpZr07ExVHV7Dgea+lMB7Xk7fNHZ+8/WhycD10G\nebALIUjh9dq/+9Hhet3XRH//pv7h7TypTEiLgYwsQI2gZfb5n+kX/+bR0QFgo8vPqdHjj391/Bd/\nPKT2l0lOBc9VBgWAi9Xwn140AtW2hJ1n+9rUetTvhusdOROx7JCnfb/2secQYRDJuT8T6a3lvjsf\nbTzDdXORB/iHb3/cLXsVNVHnGUBO5FH6pMv1wMa35/oHN9udHaWglqkIhsJJzAxijk4eVh/8ycFH\n5zPHfnMvmT78/p9cPv/o9RGyUGEgAwIzQrELYoEQZZiRBZtP7GvbeG4cP3Sb9zE1JREKNlep0tBJ\nYRcKSIkqo4rIAEZBv16EufOzEa3ND8vugjjgvXMBPjhfsR+vz4acMgHPz206sz5o8OQiXLJmUEfU\nJzJBP5Tw8z+6dFyJc/DV0PfNg79s0+KrszrWZT4HrQ1qECIQzABcmCMkDNZLm/Z7L2i7VX/gLxdz\nVuB5HPy8TwKCKUrOoarqegcWSz4HkZJBRVZLqz1NvA+sYDIido2vZsQ1uUi+ltMTU2qcfWM/V7UN\nDoUtKIWKwIg9CFYUmjEsjl/z/Chey0SVdDt08rmUue9mU52ZojPygKg5gEFEYIWRqZlSHcFz92Tz\nao/KxNRsY/MF77cNj0yLajGDwRVRU0dcuwBlVi0w1b5DEm+mICIKzkXvG3Ag9uwijA2oHKrGlIkE\nxMjBtMAJsTcAUSkK9QN6p/DrtD73480sGgKt+jL2VrFpISYjRwi4sB1gIAdLapmM+CzHFUasbFaY\naskRNoR6ZJZTvyQQYKK5SJaS1SxUzCGYC6YEU09ERJ5cZFcbeyImcjAjMjN1wNRpviBfRVgbQOZM\nLkiswAVIj9M1lj7PckpmrcbpSEO0yQYyUVILRt4TPOABp7ALhiNVZMWQjS6mqrAPk7Q6VelEz9WI\n/cSPNkAurRfMnI2RlRneei1KwaOuvZEHeSIm/k+KCwC5S5ubVXzQlnznjF/OjKAgKwwFRMFmTCQB\nUpQidWtKQmamvkIza6JtzRBqG4S6Yt4DWWGAkRnRRZbEJkRJGc4DzpTMWIUkHUk5BzluNnky52bK\nzlHl/bhxeYAKExEVykuRFql4g5IJBJJaRw6+MWayMm3C1nT05Pj0Z0fu2wPqCFIKagEGIIHYzAxK\nxmx1dKvO+jSQ6eAmI6LAWkXKRUzZTFUBISOgsChIwQIjcPBNMwGoSHFcqUoZzs2Sa7Y5TDWXYThW\nKaYZBDCMSInhPOIWuy2QeVwMNzWxXmBezJyaK+zi9sb04OTsyVqHDqFmCkpEDFJFIUABRVEykHdI\nQ+6WZ15xzJvcI0QKoAh2DoPaeuA++WI+iZ2vzLPfnSdm6UuhWIuRmkVfS0kwITeiMDWKmgZIMsmq\nGSQAzF00UMEEIjB5uKBKqmamJoNIcB7kJATs72zdfXz0/rn+9NB/Y6MwIGbFQICa6YXhSGwOYlgu\nU/fez+sXf4uv33704+jjUAGNJ4EyU+VR16oWhKqtzcrpECQlWJt5SV7NgZgpGBP7Edj9+pg5Z0pK\nySgTKQCiiwKILijDiCVpTjnnnLPkvqShF1Ui52Mzn0xuXds/y/j3n/HZOecMAamRClshFWhGFsvF\n+h7dYAc//+nR4X1/9cWnbu/wjM5WmoqmAUOmLDAScovAT2t+yPx0cGWRcLjEkkai+WJcMgfiEai2\nUtAvzAaNhsqTd0YGR2ACEzmCY/ZELF6kmMLs1wVpHqyR2ExjHInJ5d2tZ05Ofvx08S/fo7/zujUj\nS2LOlI1U6WJK94VOeyx6nN75kN/58/GzLy83bty/c18cp4TdDR2NrWoQIkIkkBmQBd3AR0/x4Umz\nmoxNklno2tYgMHVxEupdYi7pqeXWSAxiMDMhIxAbwMQXk8CbmYHMGDCQuTpOty9X1RRFTHpP+eVn\ndz6Q4d9+mjeD/xsv5tnIGCgKAxiczITMAmWH1XK5/rP/e+trsp7unvRVOU59sVZ4M2tsraldFQFW\ngeXiTs7twTHdD3tEDqogVhNmI/ZEZKrkxqAtU7OyVBMYm6ULslYUYnLec/A+jJF7tUJEVI1mG1du\n1XEsZVAk0bbvF0x6+/ntu4/iv/hs8WTg37+VNsZWRYtKg8F5OCBUsAAlW9/5aCnkd68f8VzWR8lo\nnTFf8Li2UU1VZeRIjPreDhfuXhrVV/e8kZGv4pjZEQMuOK78aBTqqchoaEPq6pK6lFsZiloCzMwA\nuBBjPfXf/CffOvjg4PGPHgyn9Xz3xRgnqgU5a16JrTABwzvHL063jx/y9w+Xj9f+y5fLW8/mWZTa\nIROy8udHvFibEayk7vMPdX0+RH+3q4Z13xvOem6iVRUcE2BEtC72qIzd/vVxqMyMXPDVGAZDIbCq\nSk5wA4jJRedHMGJ2g12YT0UhAExE1egvun+10vX9ww/vPnz0eH3atz3a4s46O13LmWLwqkQUbdHr\nwTp3mgZDGV7ZGF7f7aKXw3N/95QPjnMczKBt5qTWg7pYD+x0SNucLtVUe3MOZsZMytS6id97dnd3\nNzCbEfumme5d4MJFGAQKFEY+jkGc+5WmViWLZpUOkkyLmRqxr2Y+aj1HVS59gebbT49+PDx4wiWp\nDWzFfLHi4LwpEGBRXTdMGh/D+JFMHt5fWintqs9qWfmss8uwyikReZXYt8yuOHeK2Pd5FsipmhpF\ndqPp5VtvjUY1W0/sAQdXFWVJyWStWkyKGcDR1TPnJyrZJJkVIoCDqoEunsaiknyWVkAoucm6Z+NB\nK1GFZrhgFQNMBWpqNSyWYgOZllQ0C8SGoRixEd05Gx6fpqZ28wocQUSBjKwgiQPMUQsNTMwcff38\na99+9jd+78mnPyrtocGBHVEk78lYJUkeSk5mAhqoFBcScVRJpgMAdo6IiT2pmBVI9l15KiZ5OJHV\n3frpvdliLZ2kbCpFxKBshgItpiAxS6UrRt6BVUzFTOnJWu6d5WuONr2ZoSQlx/DGZJ5IYSAjmIGa\n+eazr3xr7+Xf4bBJYWxcXdxGglMzJXLVhJjALJJ+nY6WlqiIGRFDRcoAKgSB9AYxM788/iPpF93R\nYvH5SToocVHlzlHm3JsuB0mmYirQpGWd0IPYkXcgZ1KYaC3y6engzW5VbuxQYNm4z1YM8BzIFFAY\nKTlyV299dbr/ct/1zCtylZE3VStgJoBFxBCIR+QEIqZmKiodcTEO5IIxw2BSRAs0g1RV/MO3P1wd\nuZMH49XxlTywMSxISe1wssjrTrJIUVUQcQghxoqIYVZES8m56Gcn6bwrb1R86UJVmalBldKAVcL1\nxoWoIlxgmiUt1kRsKip9jOOOokhrIE+OyAMABTgPbxAhDCwFMHIMH9lXYF+G3lSZC4cRMxPUv/+n\n20NLJpV5ookWOc8nx3nRayqqUDEiCjEGH7z3plxKySmVUjTpItnBqnjgWuBLFfWw80xDMTOT6D9a\nyirrK1MKHsymROcPPtp7+a3x9ob3o34diWtBYvZwdVFngJFn5x05gDQvlTqVbIBnDlUTqnFuinTB\n2YhgUjrJK3/y4DTUMU57q8uwXKSTpaZiwiBynp0PTB7knHMcgjhCIWJBJ4XwaFXaYjvML43x4pau\njR8t7aCFOp7szTcern911haj12ccIzGhrJ8Ox49Gt77SLdv18jynrAIQlUJKyi7EauJ8lDyAYKpM\nCLECEcg5xyEyBiVWSYOW1spgln0nKw6cRPKTTloxBTM7X8GcFjJzAEIT4u42T8fwXgfRZVcWyycH\np4ftGQg3avfWLm1tlLMik5po4Wiy6eYbt3n6eHnvsySz1l68kMGWnvzqJ9fe/FtEAEpOay0JMFP1\nPtbjjRDHRM5idt4XH5ybOxLVlHIhB5MEHTSvy3AO7VWLafHT3bVlLR3I2DdM5Jga8mMYqRaggMU1\nEKzLaWuFLJsKwfTxWgdRB/faJr+8W7KjoLo012Ju9ViV9+bjm/ub7z86er+1sXP70cjT+eM7D3/2\n3e2X3hpPN8mylI7Z+3peT7arekPVwOTjqKqbrkfK52noTQewKZIoudq7MNE2aSGS3op69wfbjIqK\n58RI0AG6Yiyznfd0NuRFklbKqgcvmZ2yh5EIieF4VYw4wO1FatTqiDPzv8qzkzgmg6qwozeu7Szb\n7t7J6qcr+WLDz46YMTz85ffm11+ZTHdHo7kYhiET4PwIHEPl2EM1pX7R9SelPS/DCjrAilEBGQPm\njNgQjCrnqPZ0a+zX0S0ICy1ZkDP6wVa9LrIsRTuxYkYgJQ2eAIMnIinUDQXmMtmfPsVf2w5c6Ad9\nfQfRIPRrTJXowps3dhZ9Pll173YYjG6M2J7ePb77wd5LXzXnTC7QSKP3ITBXKHnRLo7Wpw9yf2ol\nmWWgEJSYzEShZBffXQxIBvWT5ZJ7Z70rg1JRKkpa1DJcgRPQr11AIhDDOwcKgJOcA/+aj37R0z++\n66/OQxg5MFjVeTKYkRphc1r/1s0rP/z40XmfftnpudB1Se0vf/Rbu8+zDzkltTyZbcRYm5V2sRi6\nJ93pwbA6MGmdY+9dyUU0M8NVlRFdfFU2VhVHpn7WiWosznHtuZTS5UIKx3AMp+qMlZiIiKEOCSKp\n5MKFrm/EPmfPeGYjbEwIwdSUVPUiySMzNuJCcFcvjb4drv3lncOjZXcn6amguvPplfsfXr/6wmQy\ndnHu/FjVhu58tTrI7Ulen5CI81WIUcWpDDAQMfMsTmYmK6Cwr1eLZVmfevfz4kPyDqxwi8yLQq2U\n1kprln4NP1BXBpZsKr1dGCPE+2OMffTMo5A9kxmJCLMRMQmIGGoKcs6RD9d25pe3tx6dtT/55MHh\n+YJWy19+8lcv3ny5Hs9N+cK1Wpwd9eeHkBWZsIvOe7W4OF+UdB5iYOdW7dFWrLwnUiErVTNr+3P/\n5JMdqrJRm1ddPkvSwgpETAtgIDDIlUQlKymIAALM1ESLVTAnrMQIZKaqZmxEysTMZA6qoiWpd+Z9\nE5sXr2zsbu59+ODhh/cffnb/k3t339/efoadj2HEvulWy6E9iawhBvZkcCmFUlxOA0xASWVoTw7H\n84ZoSeZcnI6+MP7/AVw7Vyv7vhssAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x1109F8208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = imgs[0]\n",
    "print(RESULT[labels[0].item()])\n",
    "transforms.ToPILImage()(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abs_size(x):\n",
    "    S = x.size()\n",
    "    result = 1\n",
    "    for s in S[1:]:\n",
    "        result *= s\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FishNet(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (apool): AdaptiveAvgPool2d(output_size=8)\n",
       "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FishNet(nn.Module):\n",
    "    def __init__(self,number_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.apool = nn.AdaptiveAvgPool2d(8)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 16, 128)     # 8x8 square output, 16 layers\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, number_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.apool(x)\n",
    "        x = x.view(-1, abs_size(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "mynet = FishNet(2)\n",
    "mynet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mynet.parameters(), lr=0.01, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, decay_factor=0.1):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= decay_factor\n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_stagnant(L,n=10,eps=1e-2):\n",
    "    M1 = np.mean(L[-2*n:-n])\n",
    "    M2 = np.mean(L[-n:])\n",
    "    D = (M1-M2)/(M1+M2)\n",
    "    return abs(D)<eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "#optim.lr_scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOSSES = []\n",
    "EPOCH_ELAPSED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---------- Epoch 200 ----------\n",
      "     Batch 04 | Loss: 0.0464\n",
      "     Batch 08 | Loss: 0.0984\n",
      "     Batch 12 | Loss: 0.1678\n",
      "     Batch 16 | Loss: 0.5020\n",
      "     Batch 20 | Loss: 0.3682\n",
      "     Batch 24 | Loss: 0.4399\n",
      "     Batch 28 | Loss: 0.2335\n",
      "     Batch 32 | Loss: 0.3792\n",
      "     Batch 36 | Loss: 0.4285\n",
      "\n",
      " ---------- Epoch 201 ----------\n",
      "     Batch 04 | Loss: 0.2038\n",
      "     Batch 08 | Loss: 0.3030\n",
      "     Batch 12 | Loss: 0.1550\n",
      "     Batch 16 | Loss: 0.1746\n",
      "     Batch 20 | Loss: 0.0359\n",
      "     Batch 24 | Loss: 0.6381\n",
      "     Batch 28 | Loss: 0.4234\n",
      "     Batch 32 | Loss: 0.2977\n",
      "     Batch 36 | Loss: 0.3656\n",
      "\n",
      " ---------- Epoch 202 ----------\n",
      "     Batch 04 | Loss: 0.1824\n",
      "     Batch 08 | Loss: 0.2124\n",
      "     Batch 12 | Loss: 0.1866\n",
      "     Batch 16 | Loss: 0.1029\n",
      "     Batch 20 | Loss: 0.1432\n",
      "     Batch 24 | Loss: 0.3340\n",
      "     Batch 28 | Loss: 0.2143\n",
      "     Batch 32 | Loss: 0.2060\n",
      "     Batch 36 | Loss: 0.2847\n",
      "\n",
      " ---------- Epoch 203 ----------\n",
      "     Batch 04 | Loss: 0.0766\n",
      "     Batch 08 | Loss: 0.2002\n",
      "     Batch 12 | Loss: 0.0787\n",
      "     Batch 16 | Loss: 0.0779\n",
      "     Batch 20 | Loss: 0.4105\n",
      "     Batch 24 | Loss: 0.2612\n",
      "     Batch 28 | Loss: 0.4351\n",
      "     Batch 32 | Loss: 0.2105\n",
      "     Batch 36 | Loss: 0.1201\n",
      "\n",
      " ---------- Epoch 204 ----------\n",
      "     Batch 04 | Loss: 0.1753\n",
      "     Batch 08 | Loss: 0.1721\n",
      "     Batch 12 | Loss: 0.1834\n",
      "     Batch 16 | Loss: 0.1029\n",
      "     Batch 20 | Loss: 0.0744\n",
      "     Batch 24 | Loss: 0.1312\n",
      "     Batch 28 | Loss: 0.1438\n",
      "     Batch 32 | Loss: 0.2356\n",
      "     Batch 36 | Loss: 0.1211\n",
      "\n",
      " ---------- Epoch 205 ----------\n",
      "     Batch 04 | Loss: 0.1134\n",
      "     Batch 08 | Loss: 0.1286\n",
      "     Batch 12 | Loss: 0.1576\n",
      "     Batch 16 | Loss: 0.1026\n",
      "     Batch 20 | Loss: 0.0913\n",
      "     Batch 24 | Loss: 0.1232\n",
      "     Batch 28 | Loss: 0.0886\n",
      "     Batch 32 | Loss: 0.0243\n",
      "     Batch 36 | Loss: 0.0715\n",
      "\n",
      " ---------- Epoch 206 ----------\n",
      "     Batch 04 | Loss: 0.1181\n",
      "     Batch 08 | Loss: 0.0650\n",
      "     Batch 12 | Loss: 0.2294\n",
      "     Batch 16 | Loss: 0.0676\n",
      "     Batch 20 | Loss: 0.0865\n",
      "     Batch 24 | Loss: 0.1605\n",
      "     Batch 28 | Loss: 0.1844\n",
      "     Batch 32 | Loss: 0.3251\n",
      "     Batch 36 | Loss: 0.1596\n",
      "\n",
      " ---------- Epoch 207 ----------\n",
      "     Batch 04 | Loss: 0.0995\n",
      "     Batch 08 | Loss: 0.0577\n",
      "     Batch 12 | Loss: 0.0576\n",
      "     Batch 16 | Loss: 0.1908\n",
      "     Batch 20 | Loss: 0.1817\n",
      "     Batch 24 | Loss: 0.1380\n",
      "     Batch 28 | Loss: 0.0705\n",
      "     Batch 32 | Loss: 0.1471\n",
      "     Batch 36 | Loss: 0.1526\n",
      "\n",
      " ---------- Epoch 208 ----------\n",
      "     Batch 04 | Loss: 0.1001\n",
      "     Batch 08 | Loss: 0.0485\n",
      "     Batch 12 | Loss: 0.0971\n",
      "     Batch 16 | Loss: 0.0422\n",
      "     Batch 20 | Loss: 0.0527\n",
      "     Batch 24 | Loss: 0.0624\n",
      "     Batch 28 | Loss: 0.0926\n",
      "     Batch 32 | Loss: 0.0747\n",
      "     Batch 36 | Loss: 0.0112\n",
      "\n",
      " ---------- Epoch 209 ----------\n",
      "     Batch 04 | Loss: 0.0936\n",
      "     Batch 08 | Loss: 0.0694\n",
      "     Batch 12 | Loss: 0.0313\n",
      "     Batch 16 | Loss: 0.0188\n",
      "     Batch 20 | Loss: 0.0631\n",
      "     Batch 24 | Loss: 0.0610\n",
      "     Batch 28 | Loss: 0.0879\n",
      "     Batch 32 | Loss: 0.0232\n",
      "     Batch 36 | Loss: 0.0454\n",
      "\n",
      " ---------- Epoch 210 ----------\n",
      "     Batch 04 | Loss: 0.2557\n",
      "     Batch 08 | Loss: 0.0992\n",
      "     Batch 12 | Loss: 0.1725\n",
      "     Batch 16 | Loss: 0.0875\n",
      "     Batch 20 | Loss: 0.2352\n",
      "     Batch 24 | Loss: 0.0646\n",
      "     Batch 28 | Loss: 0.2344\n",
      "     Batch 32 | Loss: 0.0677\n",
      "     Batch 36 | Loss: 0.0344\n",
      "\n",
      " ---------- Epoch 211 ----------\n",
      "     Batch 04 | Loss: 0.0546\n",
      "     Batch 08 | Loss: 0.1246\n",
      "     Batch 12 | Loss: 0.5768\n",
      "     Batch 16 | Loss: 0.2263\n",
      "     Batch 20 | Loss: 0.2320\n",
      "     Batch 24 | Loss: 0.3669\n",
      "     Batch 28 | Loss: 0.1267\n",
      "     Batch 32 | Loss: 0.0905\n",
      "     Batch 36 | Loss: 0.0604\n",
      "\n",
      " ---------- Epoch 212 ----------\n",
      "     Batch 04 | Loss: 0.0292\n",
      "     Batch 08 | Loss: 0.0870\n",
      "     Batch 12 | Loss: 0.0967\n",
      "     Batch 16 | Loss: 0.1552\n",
      "     Batch 20 | Loss: 0.0822\n",
      "     Batch 24 | Loss: 0.3105\n",
      "     Batch 28 | Loss: 0.0647\n",
      "     Batch 32 | Loss: 0.1425\n",
      "     Batch 36 | Loss: 0.0869\n",
      "\n",
      " ---------- Epoch 213 ----------\n",
      "     Batch 04 | Loss: 0.0277\n",
      "     Batch 08 | Loss: 0.0204\n",
      "     Batch 12 | Loss: 0.0860\n",
      "     Batch 16 | Loss: 0.0976\n",
      "     Batch 20 | Loss: 0.0273\n",
      "     Batch 24 | Loss: 0.3208\n",
      "     Batch 28 | Loss: 0.0634\n",
      "     Batch 32 | Loss: 0.2340\n",
      "     Batch 36 | Loss: 0.4808\n",
      "\n",
      " ---------- Epoch 214 ----------\n",
      "     Batch 04 | Loss: 0.1327\n",
      "     Batch 08 | Loss: 0.2403\n",
      "     Batch 12 | Loss: 0.0759\n",
      "     Batch 16 | Loss: 0.1098\n",
      "     Batch 20 | Loss: 0.1569\n",
      "     Batch 24 | Loss: 0.0925\n",
      "     Batch 28 | Loss: 0.0617\n",
      "     Batch 32 | Loss: 0.0124\n",
      "     Batch 36 | Loss: 0.0579\n",
      "\n",
      " ---------- Epoch 215 ----------\n",
      "     Batch 04 | Loss: 0.0469\n",
      "     Batch 08 | Loss: 0.1516\n",
      "     Batch 12 | Loss: 0.3228\n",
      "     Batch 16 | Loss: 0.1379\n",
      "     Batch 20 | Loss: 0.1379\n",
      "     Batch 24 | Loss: 0.0814\n",
      "     Batch 28 | Loss: 0.1188\n",
      "     Batch 32 | Loss: 0.1068\n",
      "     Batch 36 | Loss: 0.1205\n",
      "\n",
      " ---------- Epoch 216 ----------\n",
      "     Batch 04 | Loss: 0.0585\n",
      "     Batch 08 | Loss: 0.0516\n",
      "     Batch 12 | Loss: 0.0404\n",
      "     Batch 16 | Loss: 0.0552\n",
      "     Batch 20 | Loss: 0.0687\n",
      "     Batch 24 | Loss: 0.0416\n",
      "     Batch 28 | Loss: 0.0462\n",
      "     Batch 32 | Loss: 0.0006\n",
      "     Batch 36 | Loss: 0.1350\n",
      "\n",
      " ---------- Epoch 217 ----------\n",
      "     Batch 04 | Loss: 0.0588\n",
      "     Batch 08 | Loss: 0.3279\n",
      "     Batch 12 | Loss: 0.0663\n",
      "     Batch 16 | Loss: 0.0526\n",
      "     Batch 20 | Loss: 0.0236\n",
      "     Batch 24 | Loss: 0.0720\n",
      "     Batch 28 | Loss: 0.0373\n",
      "     Batch 32 | Loss: 0.0411\n",
      "     Batch 36 | Loss: 0.0366\n",
      "\n",
      " ---------- Epoch 218 ----------\n",
      "     Batch 04 | Loss: 0.0311\n",
      "     Batch 08 | Loss: 0.0582\n",
      "     Batch 12 | Loss: 0.0098\n",
      "     Batch 16 | Loss: 0.0164\n",
      "     Batch 20 | Loss: 0.0398\n",
      "     Batch 24 | Loss: 0.0241\n",
      "     Batch 28 | Loss: 0.0260\n",
      "     Batch 32 | Loss: 0.0171\n",
      "     Batch 36 | Loss: 0.0113\n",
      "\n",
      " ---------- Epoch 219 ----------\n",
      "     Batch 04 | Loss: 0.0196\n",
      "     Batch 08 | Loss: 0.0243\n",
      "     Batch 12 | Loss: 0.0048\n",
      "     Batch 16 | Loss: 0.0076\n",
      "     Batch 20 | Loss: 0.0097\n",
      "     Batch 24 | Loss: 0.0046\n",
      "     Batch 28 | Loss: 0.0210\n",
      "     Batch 32 | Loss: 0.0067\n",
      "     Batch 36 | Loss: 0.0137\n",
      "\n",
      " ---------- Epoch 220 ----------\n",
      "     Batch 04 | Loss: 0.0052\n",
      "     Batch 08 | Loss: 0.0056\n",
      "     Batch 12 | Loss: 0.0069\n",
      "     Batch 16 | Loss: 0.0073\n",
      "     Batch 20 | Loss: 0.0106\n",
      "     Batch 24 | Loss: 0.0065\n",
      "     Batch 28 | Loss: 0.0120\n",
      "     Batch 32 | Loss: 0.0092\n",
      "     Batch 36 | Loss: 0.0014\n",
      "\n",
      " ---------- Epoch 221 ----------\n",
      "     Batch 04 | Loss: 0.0067\n",
      "     Batch 08 | Loss: 0.0061\n",
      "     Batch 12 | Loss: 0.0033\n",
      "     Batch 16 | Loss: 0.0033\n",
      "     Batch 20 | Loss: 0.0084\n",
      "     Batch 24 | Loss: 0.0010\n",
      "     Batch 28 | Loss: 0.0165\n",
      "     Batch 32 | Loss: 0.0026\n",
      "     Batch 36 | Loss: 0.0033\n",
      "\n",
      " ---------- Epoch 222 ----------\n",
      "     Batch 04 | Loss: 0.0027\n",
      "     Batch 08 | Loss: 0.0047\n",
      "     Batch 12 | Loss: 0.0031\n",
      "     Batch 16 | Loss: 0.0063\n",
      "     Batch 20 | Loss: 0.0045\n",
      "     Batch 24 | Loss: 0.0045\n",
      "     Batch 28 | Loss: 0.0037\n",
      "     Batch 32 | Loss: 0.0041\n",
      "     Batch 36 | Loss: 0.0006\n",
      "\n",
      " ---------- Epoch 223 ----------\n",
      "     Batch 04 | Loss: 0.0111\n",
      "     Batch 08 | Loss: 0.0030\n",
      "     Batch 12 | Loss: 0.0149\n",
      "     Batch 16 | Loss: 0.0005\n",
      "     Batch 20 | Loss: 0.0040\n",
      "     Batch 24 | Loss: 0.0023\n",
      "     Batch 28 | Loss: 0.0019\n",
      "     Batch 32 | Loss: 0.0016\n",
      "     Batch 36 | Loss: 0.0042\n",
      "\n",
      " ---------- Epoch 224 ----------\n",
      "     Batch 04 | Loss: 0.0012\n",
      "     Batch 08 | Loss: 0.0011\n",
      "     Batch 12 | Loss: 0.0019\n",
      "     Batch 16 | Loss: 0.0009\n",
      "     Batch 20 | Loss: 0.0031\n",
      "     Batch 24 | Loss: 0.0056\n",
      "     Batch 28 | Loss: 0.0041\n",
      "     Batch 32 | Loss: 0.0017\n",
      "     Batch 36 | Loss: 0.0024\n",
      "\n",
      " ---------- Epoch 225 ----------\n",
      "     Batch 04 | Loss: 0.0180\n",
      "     Batch 08 | Loss: 0.0177\n",
      "     Batch 12 | Loss: 0.0004\n",
      "     Batch 16 | Loss: 0.0038\n",
      "     Batch 20 | Loss: 0.0047\n",
      "     Batch 24 | Loss: 0.0027\n",
      "     Batch 28 | Loss: 0.0022\n",
      "     Batch 32 | Loss: 0.0031\n",
      "     Batch 36 | Loss: 0.0004\n",
      "\n",
      " ---------- Epoch 226 ----------\n",
      "     Batch 04 | Loss: 0.0009\n",
      "     Batch 08 | Loss: 0.0021\n",
      "     Batch 12 | Loss: 0.0023\n",
      "     Batch 16 | Loss: 0.0025\n",
      "     Batch 20 | Loss: 0.0046\n",
      "     Batch 24 | Loss: 0.0014\n",
      "     Batch 28 | Loss: 0.0021\n",
      "     Batch 32 | Loss: 0.0051\n",
      "     Batch 36 | Loss: 0.0017\n",
      "\n",
      " ---------- Epoch 227 ----------\n",
      "     Batch 04 | Loss: 0.0030\n",
      "     Batch 08 | Loss: 0.0023\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0028\n",
      "     Batch 20 | Loss: 0.0015\n",
      "     Batch 24 | Loss: 0.0006\n",
      "     Batch 28 | Loss: 0.0007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Batch 32 | Loss: 0.0061\n",
      "     Batch 36 | Loss: 0.0019\n",
      "\n",
      " ---------- Epoch 228 ----------\n",
      "     Batch 04 | Loss: 0.0051\n",
      "     Batch 08 | Loss: 0.0015\n",
      "     Batch 12 | Loss: 0.0006\n",
      "     Batch 16 | Loss: 0.0013\n",
      "     Batch 20 | Loss: 0.0028\n",
      "     Batch 24 | Loss: 0.0015\n",
      "     Batch 28 | Loss: 0.0020\n",
      "     Batch 32 | Loss: 0.0037\n",
      "     Batch 36 | Loss: 0.0012\n",
      "\n",
      " ---------- Epoch 229 ----------\n",
      "     Batch 04 | Loss: 0.0039\n",
      "     Batch 08 | Loss: 0.0030\n",
      "     Batch 12 | Loss: 0.0006\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0015\n",
      "     Batch 24 | Loss: 0.0014\n",
      "     Batch 28 | Loss: 0.0009\n",
      "     Batch 32 | Loss: 0.0005\n",
      "     Batch 36 | Loss: 0.0010\n",
      "\n",
      " ---------- Epoch 230 ----------\n",
      "     Batch 04 | Loss: 0.0011\n",
      "     Batch 08 | Loss: 0.0008\n",
      "     Batch 12 | Loss: 0.0011\n",
      "     Batch 16 | Loss: 0.0007\n",
      "     Batch 20 | Loss: 0.0007\n",
      "     Batch 24 | Loss: 0.0024\n",
      "     Batch 28 | Loss: 0.0021\n",
      "     Batch 32 | Loss: 0.0005\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 231 ----------\n",
      "     Batch 04 | Loss: 0.0004\n",
      "     Batch 08 | Loss: 0.0011\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0020\n",
      "     Batch 20 | Loss: 0.0011\n",
      "     Batch 24 | Loss: 0.0022\n",
      "     Batch 28 | Loss: 0.0016\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0005\n",
      "\n",
      " ---------- Epoch 232 ----------\n",
      "     Batch 04 | Loss: 0.0003\n",
      "     Batch 08 | Loss: 0.0008\n",
      "     Batch 12 | Loss: 0.0016\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0006\n",
      "     Batch 24 | Loss: 0.0024\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0021\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 233 ----------\n",
      "     Batch 04 | Loss: 0.0005\n",
      "     Batch 08 | Loss: 0.0009\n",
      "     Batch 12 | Loss: 0.0019\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0006\n",
      "     Batch 24 | Loss: 0.0007\n",
      "     Batch 28 | Loss: 0.0012\n",
      "     Batch 32 | Loss: 0.0004\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 234 ----------\n",
      "     Batch 04 | Loss: 0.0014\n",
      "     Batch 08 | Loss: 0.0009\n",
      "     Batch 12 | Loss: 0.0008\n",
      "     Batch 16 | Loss: 0.0008\n",
      "     Batch 20 | Loss: 0.0009\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0006\n",
      "     Batch 36 | Loss: 0.0006\n",
      "\n",
      " ---------- Epoch 235 ----------\n",
      "     Batch 04 | Loss: 0.0010\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0007\n",
      "     Batch 16 | Loss: 0.0006\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0011\n",
      "     Batch 32 | Loss: 0.0003\n",
      "     Batch 36 | Loss: 0.0016\n",
      "\n",
      " ---------- Epoch 236 ----------\n",
      "     Batch 04 | Loss: 0.0008\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0006\n",
      "     Batch 16 | Loss: 0.0004\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0017\n",
      "     Batch 28 | Loss: 0.0008\n",
      "     Batch 32 | Loss: 0.0006\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 237 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0004\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0003\n",
      "     Batch 20 | Loss: 0.0007\n",
      "     Batch 24 | Loss: 0.0013\n",
      "     Batch 28 | Loss: 0.0004\n",
      "     Batch 32 | Loss: 0.0006\n",
      "     Batch 36 | Loss: 0.0008\n",
      "\n",
      " ---------- Epoch 238 ----------\n",
      "     Batch 04 | Loss: 0.0007\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0009\n",
      "     Batch 16 | Loss: 0.0004\n",
      "     Batch 20 | Loss: 0.0003\n",
      "     Batch 24 | Loss: 0.0007\n",
      "     Batch 28 | Loss: 0.0009\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0004\n",
      "\n",
      " ---------- Epoch 239 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0003\n",
      "     Batch 12 | Loss: 0.0003\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0006\n",
      "     Batch 24 | Loss: 0.0006\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0012\n",
      "     Batch 36 | Loss: 0.0009\n",
      "\n",
      " ---------- Epoch 240 ----------\n",
      "     Batch 04 | Loss: 0.0004\n",
      "     Batch 08 | Loss: 0.0005\n",
      "     Batch 12 | Loss: 0.0003\n",
      "     Batch 16 | Loss: 0.0004\n",
      "     Batch 20 | Loss: 0.0004\n",
      "     Batch 24 | Loss: 0.0007\n",
      "     Batch 28 | Loss: 0.0003\n",
      "     Batch 32 | Loss: 0.0008\n",
      "     Batch 36 | Loss: 0.0004\n",
      "\n",
      " ---------- Epoch 241 ----------\n",
      "     Batch 04 | Loss: 0.0005\n",
      "     Batch 08 | Loss: 0.0005\n",
      "     Batch 12 | Loss: 0.0005\n",
      "     Batch 16 | Loss: 0.0004\n",
      "     Batch 20 | Loss: 0.0010\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0003\n",
      "     Batch 32 | Loss: 0.0004\n",
      "     Batch 36 | Loss: 0.0003\n",
      "\n",
      " ---------- Epoch 242 ----------\n",
      "     Batch 04 | Loss: 0.0004\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0003\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0010\n",
      "     Batch 24 | Loss: 0.0004\n",
      "     Batch 28 | Loss: 0.0004\n",
      "     Batch 32 | Loss: 0.0004\n",
      "     Batch 36 | Loss: 0.0004\n",
      "\n",
      " ---------- Epoch 243 ----------\n",
      "     Batch 04 | Loss: 0.0003\n",
      "     Batch 08 | Loss: 0.0013\n",
      "     Batch 12 | Loss: 0.0006\n",
      "     Batch 16 | Loss: 0.0004\n",
      "     Batch 20 | Loss: 0.0006\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 244 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0007\n",
      "     Batch 12 | Loss: 0.0007\n",
      "     Batch 16 | Loss: 0.0004\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0007\n",
      "\n",
      " ---------- Epoch 245 ----------\n",
      "     Batch 04 | Loss: 0.0006\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0005\n",
      "     Batch 24 | Loss: 0.0005\n",
      "     Batch 28 | Loss: 0.0007\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0004\n",
      "\n",
      " ---------- Epoch 246 ----------\n",
      "     Batch 04 | Loss: 0.0003\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0005\n",
      "     Batch 16 | Loss: 0.0005\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0005\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0005\n",
      "     Batch 36 | Loss: 0.0004\n",
      "\n",
      " ---------- Epoch 247 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0005\n",
      "     Batch 12 | Loss: 0.0008\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0004\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0004\n",
      "     Batch 36 | Loss: 0.0003\n",
      "\n",
      " ---------- Epoch 248 ----------\n",
      "     Batch 04 | Loss: 0.0004\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0005\n",
      "     Batch 24 | Loss: 0.0006\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0007\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 249 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0003\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0003\n",
      "     Batch 24 | Loss: 0.0007\n",
      "     Batch 28 | Loss: 0.0005\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 250 ----------\n",
      "     Batch 04 | Loss: 0.0003\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0005\n",
      "     Batch 16 | Loss: 0.0004\n",
      "     Batch 20 | Loss: 0.0006\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0003\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 251 ----------\n",
      "     Batch 04 | Loss: 0.0004\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0003\n",
      "     Batch 20 | Loss: 0.0004\n",
      "     Batch 24 | Loss: 0.0003\n",
      "     Batch 28 | Loss: 0.0005\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 252 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0003\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0009\n",
      "\n",
      " ---------- Epoch 253 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0005\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0005\n",
      "     Batch 32 | Loss: 0.0003\n",
      "     Batch 36 | Loss: 0.0006\n",
      "\n",
      " ---------- Epoch 254 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0004\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0005\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0004\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 255 ----------\n",
      "     Batch 04 | Loss: 0.0005\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0004\n",
      "     Batch 16 | Loss: 0.0003\n",
      "     Batch 20 | Loss: 0.0002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0003\n",
      "\n",
      " ---------- Epoch 256 ----------\n",
      "     Batch 04 | Loss: 0.0004\n",
      "     Batch 08 | Loss: 0.0006\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0004\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 257 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0006\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0004\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0003\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 258 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0005\n",
      "     Batch 20 | Loss: 0.0003\n",
      "     Batch 24 | Loss: 0.0003\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0004\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 259 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0004\n",
      "     Batch 12 | Loss: 0.0004\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0004\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 260 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0004\n",
      "     Batch 24 | Loss: 0.0004\n",
      "     Batch 28 | Loss: 0.0003\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 261 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0004\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0004\n",
      "\n",
      " ---------- Epoch 262 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0006\n",
      "     Batch 32 | Loss: 0.0003\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 263 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0008\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 264 ----------\n",
      "     Batch 04 | Loss: 0.0005\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0005\n",
      "\n",
      " ---------- Epoch 265 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0004\n",
      "\n",
      " ---------- Epoch 266 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0003\n",
      "     Batch 32 | Loss: 0.0003\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 267 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0003\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0003\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 268 ----------\n",
      "     Batch 04 | Loss: 0.0004\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0003\n",
      "\n",
      " ---------- Epoch 269 ----------\n",
      "     Batch 04 | Loss: 0.0003\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0003\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0003\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 270 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0003\n",
      "\n",
      " ---------- Epoch 271 ----------\n",
      "     Batch 04 | Loss: 0.0004\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0003\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 272 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0004\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0004\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 273 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0003\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 274 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0004\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 275 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 276 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0003\n",
      "     Batch 12 | Loss: 0.0004\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 277 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0003\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 278 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 279 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0003\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 280 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 281 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 282 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0003\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 283 ----------\n",
      "     Batch 04 | Loss: 0.0003\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0003\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 284 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 285 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 286 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 287 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0003\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 288 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 289 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 290 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 291 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 292 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 293 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 294 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 295 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 296 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0003\n",
      "\n",
      " ---------- Epoch 297 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 298 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 299 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 300 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 301 ----------\n",
      "     Batch 04 | Loss: 0.0002\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 302 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 303 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 304 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 305 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 306 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 307 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 308 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 309 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 310 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 311 ----------\n",
      "     Batch 04 | Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 312 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 313 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0002\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 314 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 315 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 316 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 317 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 318 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 319 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 320 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 321 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 322 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 323 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 324 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 325 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 326 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 327 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 328 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 329 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 330 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 331 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ========== LR decayed by 5 ========== \n",
      "\n",
      "\n",
      " ---------- Epoch 332 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 333 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 334 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 335 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 336 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 337 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 338 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 339 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 340 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 341 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 342 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 343 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 344 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 345 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 346 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 347 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 348 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 349 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 350 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 351 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 352 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 353 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 354 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 355 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 356 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 357 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 358 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 359 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 360 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 361 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ========== LR decayed by 5 ========== \n",
      "\n",
      "\n",
      " ---------- Epoch 362 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 363 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 364 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 365 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 366 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 367 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 368 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 369 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 370 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 371 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 372 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0002\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 373 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 374 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 375 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 376 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 377 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0002\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 378 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 379 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 380 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 381 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0002\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 382 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 383 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 384 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0002\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 385 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0002\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 386 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 387 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 388 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 389 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 390 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 391 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 392 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 393 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 394 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0001\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 395 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 396 ----------\n",
      "     Batch 04 | Loss: 0.0000\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0001\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0001\n",
      "     Batch 36 | Loss: 0.0001\n",
      "\n",
      " ---------- Epoch 397 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0000\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0002\n",
      "\n",
      " ---------- Epoch 398 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0000\n",
      "     Batch 12 | Loss: 0.0001\n",
      "     Batch 16 | Loss: 0.0000\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0001\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0000\n",
      "\n",
      " ---------- Epoch 399 ----------\n",
      "     Batch 04 | Loss: 0.0001\n",
      "     Batch 08 | Loss: 0.0001\n",
      "     Batch 12 | Loss: 0.0000\n",
      "     Batch 16 | Loss: 0.0002\n",
      "     Batch 20 | Loss: 0.0000\n",
      "     Batch 24 | Loss: 0.0001\n",
      "     Batch 28 | Loss: 0.0000\n",
      "     Batch 32 | Loss: 0.0000\n",
      "     Batch 36 | Loss: 0.0001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print_batch = 4\n",
    "N_epoch = 200\n",
    "epochs_since_last_decay = 0\n",
    "decay = 5\n",
    "\n",
    "for epoch in range(1,N_epoch+1):  # loop over the dataset multiple times\n",
    "    \n",
    "    print('\\n','---------- Epoch {:03d} ----------'.format(EPOCH_ELAPSED))\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainset, 1):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = mynet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item() #loss.data[0] deprecated in newer versions of PyTorch\n",
    "        if i % print_batch == 0:    # print every few mini-batches\n",
    "            LOSSES.append(running_loss)\n",
    "            print('     Batch {:02d} | Loss: {:.4f}'.format(i ,running_loss / print_batch))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    # Decay LR when learning is stalling, for better convergence\n",
    "    if epochs_since_last_decay>5 and is_stagnant(LOSSES,3):\n",
    "        optimizer = adjust_learning_rate(optimizer,decay_factor=1.0/decay)\n",
    "        epochs_since_last_decay = 0\n",
    "        print('\\n','='*10,'LR decayed by',decay,'='*10,'\\n')\n",
    "    \n",
    "    # Some updating\n",
    "    epochs_since_last_decay += 1\n",
    "    EPOCH_ELAPSED += 1\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8VOW9x/HPLyuBsCkYKKCIYhF3\nSHEtBuuC1NYueqv2ulB7uVpt623tdatbe1vr7a299Wprcd8q1K1SFxSViAuy7yKy7/sWEhIgyXP/\nmJMwmUxmJpNZT77v1ytw5pznnPPNycxvzjxzFnPOISIi/pKT7gAiIpJ4Ku4iIj6k4i4i4kMq7iIi\nPqTiLiLiQyruIiI+pOIuIuJDKu4iIj4UtbibWQczm25m88xskZndG6ZNoZmNN7NlZjbNzPonI6yI\niMQmL4Y2+4BznHOVZpYPfGRmbznnPg1qcy2w0zl3tJldBtwPfC/SQnv06OH69+8fV+iqqio6deoU\n17ypli1ZlTPxsiWrciZWsnPOmjVrm3OuZ9SGzrmYf4COwGzg1JDxbwOne8N5wDbAIi1r6NChLl6T\nJ0+Oe95Uy5asypl42ZJVORMr2TmBmS6Geh1Tn7uZ5ZrZXGALMMk5Ny2kSR9grfdmUQvsBg6NZdki\nIpJ45lpx4TAz6wa8CvzYObcwaPwi4ALn3Drv8XJgmHNue8j8Y4AxACUlJUPHjRsXV+jKykqKi4vj\nmjfVsiWrciZetmRVzsRKds4RI0bMcs6VRm0Yy+69a9oFczdwc8g4dcu0IFuyKmfiZUtW5UysrOmW\nMbOe3h47ZlYEnAt8HtJsAnC1N3wJ8L4XQkRE0iCWo2V6A0+bWS6BQyf/7px73cx+ReAdZALwOPCs\nmS0DdgCXJS2xiIhEFbW4O+fmA6eEGX9X0HANcGlio4mISLx0hqqIiA9lZXFfX1nPtBXbozcUEWmn\nsrK43/FRNd8b+2n0hiIi7VRWFncREYlMxV1ExIeyuri/s2hTuiOIiGSkrC7uY56dxZJNe9IdQ0Qk\n42R1cQf4ZPm2dEcQEck4WV/c7/3nZ+mOICKScbK+uIuISHNZV9zVDSMiEl3WFffq/XXNxu2s2p+G\nJCIimSvrinvPzoXNxm1XcRcRaSLrivvxX+rabNxrc9enIYmISObKuuKek2PNxv3f+8vSkEREJHNl\nXXEH+N8RRemOICKS0bKyuHcrbB779lcXpCGJiEhmysriDnD5sH5NHv9t2po0JRERyTxZW9zv+86J\nrLxvVLpjiIhkpKwt7gBmzb9cFRGRLC/uIiISnoq7iIgPZX1xv+aM/umOICKScbK+uOfnqt9dRCRU\n1hf3vNys/xUkA721YCPLtuguX5K9olZGM+tnZpPNbLGZLTKzn4ZpU2Zmu81srvdzV3LiNvfizHWp\nWpW0I9c/P5tzH5iS7hgiccuLoU0t8HPn3Gwz6wzMMrNJzrnQWyB96Jy7KPERI9u1V1eEFBEJFXXP\n3Tm30Tk32xveAywG+iQ7WKyKCnLTHUFEJOO0qsPazPoDpwDTwkw+3czmmdlbZnZcArLFxLlUrUlE\nJHuYi7E6mlkx8AHwG+fcKyHTugD1zrlKMxsF/Mk5NzDMMsYAYwBKSkqGjhs3Lq7QlZWVFBcXA3Dd\npCpqvJszPTWyU1zLS6bgrJlMOZu6ZmIV0LbnlLZpYilnwIgRI2Y550qjNnTORf0B8oG3gZ/F2H4V\n0CNSm6FDh7p4TZ48uXH4ysenuSNued0dccvrcS8vmYKzZjLlbCoRzylt08RSzgBgpouhDsdytIwB\njwOLnXMPtNCml9cOMxtGoLtneyzvQm1107nNPiCIiLR7sRwtcyZwJbDAzOZ6424HDgdwzj0CXAJc\nb2a1QDVwmfcOk3S5QRcPe2fRJs4/rlcqVisiktGiFnfn3EdAxNNAnXMPAQ8lKlS8Hpj0hYq7iAg+\nOENVRESaU3EXEfEhXxV33bxDRCTAV8VdREQCfFXctd8uIhLgq+IuIiIBKu4iIj7kq+Kua4hJKpQv\n2cJv31yc7hgiEfmruOsSkZIC1zw5g7FTVqQ7hkhEviruIiISkPXFvU/3onRHEBHJOFlf3HsUFzYO\nq1dGRCQg64t7sCWb9zDozrfSHUNEJO18VdwBag7UpzuCiEja+a64i4iIiruIiC+puIuI+JCKu4iI\nD6m4i4j4kIq7iIgPqbiLiPiQiruIiA+puIuI+JCKu4iID6m4i4j4UNTibmb9zGyymS02s0Vm9tMw\nbczMHjSzZWY238yGJCeuiIjEIi+GNrXAz51zs82sMzDLzCY55z4LanMhMND7ORX4i/e/iIikQdQ9\nd+fcRufcbG94D7AY6BPS7GLgGRfwKdDNzHonPK1ICmypqEl3BJE2a1Wfu5n1B04BpoVM6gOsDXq8\njuZvACJZYdhv30t3BJE2i6VbBgAzKwZeBm5yzlWETg4zS7P7IpnZGGAMQElJCeXl5bEnDVJZWRlx\n3niXmwzRsmYK5QyvLc8zbdPEUs7Wiam4m1k+gcL+vHPulTBN1gH9gh73BTaENnLOjQXGApSWlrqy\nsrLW5gUCL6om8058o8n04cPPJicn8H5z3gMf8I2TvsRPvjYwrnW1VbOsGUo5gwQ9n8Kuy5seLYe2\naWIpZ+vEcrSMAY8Di51zD7TQbAJwlXfUzGnAbufcxgTmbJVFGw5+sFi6pZIHJn2RrigiImkRy577\nmcCVwAIzm+uNux04HMA59wjwJjAKWAbsBUYnPmrsXPMeIRGRdiVqcXfOfUT4PvXgNg64IVGhRLLB\nxt3VLN1cyfBjeqY7ikgzvjxD1WnHXVLgogc/4qonpqc7hkhYvijulw7tm+4I0g5tr9qf7ggiLfJF\ncQ+lHXcRae98Udwt4jcCIiLtjy+Ke6hvPfwxy7dWpjuGiEja+KK4W5iDeZ6dujoNSUREMoMvinuO\nL34LEZHE8UlZVKe7iEgwnxR3EREJpuIuIuJDvi3u73++Jd0RRETSxhfFPdxx7mt27E19EBGRDOGL\n4i6SLP1vfYMNu6rTHUOk1XxR3PNzdLSMJM+nK7anO4JIq/mjuOf64tcQEUkYX1TFLkX56Y4gIpJR\nfFHcxwwfEHb8w5OXpTiJ+JHuDyDZyBfFvUN+btjxv397SYqTiIhkBl8U93j898TP6X/rG9TXa7dM\nRPyn3Rb3sVNWAFCnz9wSRZ12ACQLtdviLhIr7QBINlJxFxHxIRV3kSi04y7ZqN0Ud+ccizdWpDuG\niEhK5KU7QKrc/upCXpi+hj7dithauQ/tjImIn7Wb4v7C9DUArPcuApWr69GIiI9F7ZYxsyfMbIuZ\nLWxhepmZ7Tazud7PXYmPmXgq7RIrp895koVi2XN/CngIeCZCmw+dcxclJFGK6OUqIn4Wdc/dOTcF\n2JGCLGmhIyFExI/MxVDdzKw/8Lpz7vgw08qAl4F1wAbgZufcohaWMwYYA1BSUjJ03LhxcYWurKyk\nuLi4ybhrJla1ahk5BvUOHju/I3lJ7H8PlzUTKedBoc+lqwYXcM7h+S1Of2pkp7DL0TZNLOUMGDFi\nxCznXGm0don4QnU2cIRzrtLMRgH/AAaGa+icGwuMBSgtLXVlZWVxrbC8vJzQeQsmvcX+uvqYl2Fm\n4BzDh59NQV7yjggNlzUTKWeQiW80eXjMMcdQdtoRLU5vKY+2aWIpZ+u0uao55yqcc5Xe8JtAvpn1\naHOyVnrlR2fENZ++LBMRP2pzcTezXmaBW1Sb2TBvmSm/L1nHgvCX/RURaY+idsuY2QtAGdDDzNYB\ndwP5AM65R4BLgOvNrBaoBi5zsXTkJ5j2vyVZ9NySbBS1uDvnLo8y/SECh0pmJR0tIyJ+5Jtry+ik\nJEka7QFIFvJNcRcRkYNU3EWi0H67ZCPfFHe9ACVZ1Csj2cg3xV1ERA5ScReJIg1H9oq0mW+Ke7xH\ny+h1K23lnOPRKSvYUlGT7igijXxT3Furrl5VXWLjgNFPTufVOevCTp+5eie/eXMx1z8/O7XBRCLw\nTXFXqZZkcQ4mL9nKf4yfF3b6pY9MBWBPzYFUxhKJyDfFPV66cJiI+FG7L+4i0ejtX7JRuy/uB2r1\n0hUR//FNcY/3aJmTfvVOQnOI/8R6KKSOvJJM4pvirteVpJueg5JJfFPcRUTkIBV3EREfUnEXiUJ9\n6ZKNVNxFRHxIxV0kCp3oJtlIxV0kQXT1SMkkvinu3Yry0x1BfCrWa8yptEsm8U1xP7S4kKdGfyXd\nMcSHtEMu2cg3xR3gS92K0h1BfKhe1V2ykK+Ke7yXIBAR8Rt/Ffc2VvfV26sYN31NYsKIb9Trxi6S\nhaIWdzN7wsy2mNnCFqabmT1oZsvMbL6ZDUl8zFi1rbp/6+GPufWVBQnKIn5R0qVDbA31HiAZJJY9\n96eAkRGmXwgM9H7GAH9pe6z4tHXPfefewJ10dEibBOveqaBx+N3PNqcxiUjsohZ359wUYEeEJhcD\nz7iAT4FuZtY7UQFbI1F97qrtEiz4zf6Hz8xsuV0qwojEKC8By+gDrA16vM4btzG0oZmNIbB3T0lJ\nCeXl5XGtsLKyMuy8m6rq41pe6LLKPygnp60fAzwtZc00ytmyBQvD9kg2s3fv3ibZtE0TSzlbJxHF\nPVwVDLsT45wbC4wFKC0tdWVlZXGtsLy8nHDzrtpWBR+Wt3p5jcua+AYAZ59dRm5OYop7S1kzjXIG\n8Z4HDY477jiYMzvqbB07dmySTds0sZSzdRJxtMw6oF/Q477AhgQst9UStLOtPndpQk8HyUaJKO4T\ngKu8o2ZOA3Y755p1yaSCJajXPfS1vHbHXj5Zti0hy5bss3jTnsbhRO1AiCRb1G4ZM3sBKAN6mNk6\n4G4gH8A59wjwJjAKWAbsBUYnK2w0yXrhnf37ydQ7WPW7rydnBZLRHnxvaeNwpL14feKTTBK1uDvn\nLo8y3QE3JCxRBgh9jeocFhHJNr46QzVRdP1uiYeeNZJJVNzDiPTperd3opOISCZTcQeuf25Wk8fj\nZ6wN2+71+Rs46VfvMHftrlTEEhGJm4o78NbCTfzo+YMF/u4Ji5i2Ynuzdh8vC4xbtGF3yrJJ9tD3\nqZJJVNw9by7Y1OTx98Z+yj/mrGfmqkhXXhARyUyJOEM1Y/U/tCOrtu+Ne/6bxs9NYBoRkdTx9Z77\n6Uf1SHcEEZG08HVxF0klHUIrmUTFPU4vz1rHup3xd/mIiCSTr/vck+WOVwOXgO3TrYiPbz2ncXzN\ngTrKl2xl5PG90hVNRATw+Z77TecOTOryt+ypOThcUcPoJ2dw3XOzwh5GKf6nQyElk/h6zz3me18m\nwFn3T2Z/XeBmIbuqdRariKSXr4t7sjkHN/xtNlX7ahsLe8N4EZF08lVxT8e1tt+YH+7S9aruIpJe\nvu5zTzaVcAmmT2ySSXxV3Lt3LEjq8l+YviapyxcRSRRfFfdOhXmsvG9UytbX0p13tAcnIunmq+IO\nYF7He+fCzP06wTnHxIUbqQ36ElZEJJF8V9wBHr2qlDd/+tWkr6elHfTQ8TNW7eBv09aws2o/AJM+\n28x1z83mz+XLk5pPRNqvzN29bYPzBpekO0ITlz4yFYDnp63mjZ98le1ekd+4uzqdsUTEx3y5554q\nLfWttzR+0YaKFtst2bSH+95a3GI/vohIa6i4J0G0qwM2HI4ffOLTFY9+yl8/WMHOMPdovXzsp1z3\n7Kxm4yWz6I1ZMonvi/uxvbukfJ3RXuMNJ1u9Mnt947j6CDNNXbGdiYs2tThdRCSU74t7NthXW8du\n73o02vsTkUTwfXFPR7Fs7RpHPzmDetX0rKc/oWSSmIq7mY00syVmtszMbg0z/Roz22pmc72fHyY+\nqn99svzgJYIbCsT+2npqDtSlJ5DEZYd3FJRIJoh6KKSZ5QIPA+cB64AZZjbBOfdZSNPxzrkbk5Ax\n60T7tGC0fIWzeufYUbWf7/11Kku3VLLqd19PdDxJkn21OilNMkcsx7kPA5Y551YAmNk44GIgtLhL\nGNW1kQv9rr1N9/aenbqa/3t/WTIjiUg7EEu3TB9gbdDjdd64UN81s/lm9pKZ9UtIugT497MHpHyd\nU5dvp6LmAP/2zEyufzfMfVaDdtxP/tWkJpM++GJrktM19+LMtazdo71OET+JZc89XB9C6O7oP4EX\nnHP7zOw64GngnNCZzGwMMAagpKSE8vLy1qX1VFZWxjxv97jW0DbjZqxl6pL1rK5oXjDLy8tZsq7l\nOzXtrtjTrH3wcG29Y8G2Ok7umdt4HZ22+sXEKgD6dS6P3DADtOZvH4+2fgEfnC3ZWRNFORMrU3LG\nUtzXAcF74n2BDcENnHPBNw19FLg/3IKcc2OBsQClpaWurKysNVkblZeX06p5J74R13raIlxhBygr\nK2PLzLWwcH7Y6cXFxVBR0aR9Q/7d3QayfGsVD85eypPXfIWyQYclJqy3/Hj/HqnU6r99K9XVO3j7\nzbjnD86W7KyJopyJlSk5Y+mWmQEMNLMjzawAuAyYENzAzHoHPfwmsDhxEdvuO6eE60VKn/98KXxh\nj+an4+by0sxAD9l2HZmRFDrPQPwianF3ztUCNwJvEyjaf3fOLTKzX5nZN71mPzGzRWY2D/gJcE2y\nAsfF67349cXHkZeThnvxBXl4cuQvSytqIt9ce09NLdC0r+zfnpnJ36a1/UYiUxLQ3++cy+pDOFXa\nxS9iOs7dOfemc+4Y59xRzrnfeOPucs5N8IZvc84d55w7yTk3wjn3eTJDx6uoII++3YvSmuH3by+J\nOH3tjshXimwoPmYweckWdlTtZ9Jnm7n91QVtznbVE9P5ZNm2Ni3jsQ9XMujOiWzds6/NeVLt3n8u\n4tU566M3FMkCvrzkbyTZtmcWekOPhm6Dqv11/OzvMzipX7fGaQvW7eaEvl3DLuef8zZw6oBDOKxz\nh4jr21rZtqL82rxAcdy0u4aenQvbtKxUe/LjVemOIJIwvr/8gN9U7Q90eezdF+ieWbm1snHaNx76\nKOw8FTUH+PELcxj2m/d4duoqtlXuo3p/9nadiEh07aq4Z+OXZcu3VoUdf99bgZ6vSq/IB1u7Yy/9\nb32Dj5dt4+8z1rJpd03jtDtfW0Tpf73L8N9PZuLCjckJTfTLHifSz8bP5Yz73mPW6p0pW6dIpmtX\nxT0bXfC/UyJOD3fBsZmrdwDwp3eX8p8vz+f8PzZfxtY9+7juudlUhXlzaPDF5j1c+KcPm3zJu7mi\nhr/PWNviG2WkSyskyytz1rNhdw3f/csnKV+3SKZqf33u2bfz3mo53slN+2qjd72s3Bb+kwHAA+98\nweKNFXy8dBsXnhA42vXU374XmGjwL6UZcyKyiIRoF3vuwXuTDd0FPYoL0hUn6R75YAUA89btjtr2\nov9r2k/f0pvfrr37eeKjlY2PH52ygj+9u7TV2aZ8sZXHPlzRZNzC9buZ5X3aSLYNu6p5ZfY61u+q\n5v3PN6dknSLp0K723IPr1nmDS3hh+toW22arZ6auYvHGiqjtWnLT+Ll8vmkPt4z8cpPxt7w8n7cX\nHSyGS7dU8sd3v2Dm6h08dnUphXm5Tdq/Pn8jizZUcPmww6mrd2yqqKFPtyKuemI6AD/86sFr/jS8\nwcRzBcyXZ60LO37LnhrmrNnFBcf1ajL+e2OnsnZHNUX5uVQfqNNVN8W32lVxB/93y9z12qI2L+OR\nD5ZzxKEdm4zbFebergAfLt3GwvW7GXrEIazfVc2C9YFPC2OnBPbOb3tlAf8+fAB/nbKCS4f2bVWO\nO15dwOTPt/DJbV+jen8d+bmBT2Bz1uyken8dZxzdg5+/OC/svFc8Oo1lWyr56JYR9O1+8HfZXBE4\n1LM65ESrf3lkaquyiWS6dtEtI633ecje/7SVLXebOAcfLd3Gmb97P+z0KUsDJ0a9GLSXvXp7y339\na7bvZfKSLTw/bQ0bvCN9jr1rIqOfmgHAt//8CVc8No3Za1o+OmbN9sDVOM+6f3LUM2bfWbSJ6atS\n0y0kkirtorh/dWAPAI7tlfqbZWerp6eupiaGL2QBHpq8jH99fFqL08MdP3P278ubjWu4k9GIP5Qz\n+skZjeP3eEfrfLi06dmz3/lz86NjtlTUNBv3p/eWsqLhfICQT26/fXMxY56dFTZ3x4Jc5tx5Xthp\nIpmuXRT3b53Shzl3nscJfbty9zeOo2fnQo7trUIfTfmS2K41E63dZxG+Axj95PTG4TrvuM66kOM7\nT7jnnZhyALzz2Wae+nhlk3F/KV/OOX/4AOdcs+PvG7qPwnEOunfy7xfv4m/tps+94UV63uASzhtc\nQn29S0j/dHswM0knB1Xvr2Ny0BtDLCc+3Ts18rV3fvmPhS1OW7alssVpIn7TLvbcw8nJMR7516Hp\njpEVHv9oZfRGcTj2rolNHk9bsYP+t0a+9v7K3fHfMeq8P07hQF3s36jHc5ZtNl8RU/yl3RZ3gJHH\n94reSFLmxy/MSXeEJuI5surtRZsSH0QkDu26uItEEs9Rs4m69aFIW6m4i7Qkjuqu0i6Zot0X9z9c\nehIvX38GXTq0m++WJUbx9LnnaM9dMkS7L+7fHdqXoUd05/2by/jJOUenO45kkHj63Fds1RE5khna\nfXFv0KO4kEE69l0iGHJ4t6ht/jDpixQkEYlOxb2VHruqNN0RJEUev+YrTR7n5ejlItlDz9ZWOndw\nSbojSAr8qOwozj6mZ5NxuTnqT5fsoeIepFfXlm8efUxJMW/99KspTCPpFO4ciODiPuLLPZtNF8kk\nKu5BhhzenZevP6PZ+D9/fwgvXndGs+vR9CguTFU0SbFeXQ6+0Y/4ck9O6NOVnKDiHnw9+lC7q8Nf\nHlkklVTcQww9ojs3nTuQf9xwJk//YBgv/NtpjDqhN12L8pu1nfnLcxuHx485jem3fy2mdXz9xN4R\np+flGIfqglVp1bPzwTfuJ0cP458/PovcoF6Zow8rbnHeG/82m5Xbqqh3jptfnMfC9dHviCWSaCru\nYdx07jGc3K8bZx/Tk9OPOjRsm7KQj+WnDjiUw7qE79Y599iSJsMPXzGEa886snFcv0OKmrQvzMth\nVgIuNVuQG/7P+/qPz2rzsv0u3JmmDd0yv/7W8ZR06dDiXZw+XLqNEf9TzrVv7+WlWeu47rnwlxQW\nSaaYiruZjTSzJWa2zMxuDTO90MzGe9OnmVn/RAfNJAvuOZ9HWzhq5tcXH9dsXJ5XFH5xwZf58/eH\nABD83VyngoMnUHUuzOPubwaW8dtvn9BkObddOKhxeOV9oyi/uaxZruA9/mN7dw6b8fg+XfnFBV8O\nOw3g7m8M5vBDOjYb33Bd/AaDeoVfPsBRPTu1OC1W936z+bZMpy4dAp/e+nUvitIyoOEw+b37dTEx\nSb2oxd3McoGHgQuBwcDlZjY4pNm1wE7n3NHAH4H7Ex00k3TukE++t1f8pa4dmhS9K0/vz7s/G87D\nVwzhuWtP5Zendmg8wubC43tRkBeY7+xjDmuc55TDu1F+cxmf3HoOC+69gH8p7QfA5cP68afLTm5s\nN+qEQHfORSf2xszo36MTc+48j0G9OlN+cxmdO+Tz7s/Obmx//yUnNma7+vQjvPUGPnHcMOLoJsse\n0PXgU2H0mUcy5T9HsOp3X+eHQZ8wngg5NHDiTcMbhy8f1o+HrjiFeXefz6rffZ33fl7WpO35EY4y\nuv+7JzQb9/AVQ7j6jP48elVp45vIg5efws2lhaz63df5ydcGcvuoQc3m+/6phwPwjxvObBw3fsxp\njcOhb1Dh3DKy+XIB7rn4OG4fNajJUTTT7/gaw/ofEnF5O6r28/Qnq1i6eU/UdYskSizn3A8Dljnn\nVgCY2TjgYuCzoDYXA/d4wy8BD5mZOef3O5bCJ7c172c/+rDOHH1YoCDVrs+lbGhfLjqxNx3yD95E\n+qyBPVjyXyNZsbWKAT07NbvBNAS6Bi4+uQ8bdtVwVM9O9DukI/PuOp+OhQfbdu9U0KTIdu9UwFOj\nv0JBbg6DenXh2WtPpaLmAMUFefxi5CAK8w4W8YtP7sP/vLOEtTuqufO0Dvx+fl6zG2v88qLBPOZd\n8rfhDa1HcSGv3RgonrddOIhZq3dy33dObJZ/QM9OrNhaRfnNZfTv0YmaA3UMunMih3UuZPod5/If\n4+fy6pz15OfmcM83BrNyWxXdOxVw+oBDOXVAoDus4fr7tXX15OXmUL4zcJLQz847BoAxw49qvEzw\nHy49ie8O7ctvvn1C412dRp3Qi2FHHiy+PzjrSP7yr0PZXFHDxl015OcafboXUXOgniMO7Ui9c2H/\nFhDYcx8z/Kgm4w7r3IEuYb6PCXX3hMC9A3p16UBtvaOi+gB/vXIo/Q4pYu2OagZ/qQsdC3IpyMth\nX209e/fVUdKlEDOjvt7R0EtkZtTVu4iHZTa010XM2jeLVn/N7BJgpHPuh97jK4FTnXM3BrVZ6LVZ\n5z1e7rXZFm6ZAKWlpW7mzJlxhS4vL6esrCyueVMt07Pu3nuAtTv3sm3pHM786nDq6l2TNyEI3Lou\nPzeH7p0K2Li7mq5F+XQsiL5fsHJbFS/NWsvN5385bKFZt3Mvt7+6kD9/fwjFhbFd26c123Pe2l0c\nU9KZooJcKvfVMn7GWn5wZv+EF727X1vI01NXM/bKoTz4/lIWrm/5zlNt1aO4gG2V+5uN794xn9yc\nHPbUHGBfbT1F+bkUFeRSlJ9L8LlXe/fVkZdr1DvokJ9DrhkVVdXk5OVTVJCLEXjzCLw5QL13+fyG\nZRgW6Ga0ppdn2F9bT16u4Vzgmjx5OTkRL6LmgHrnMA6+CVnjP0Ht3MF2NdXVdOzYMexy6upd4Dsm\nC7y55eYYzps/XI0L9xxoNibMLxA6qmEdBtQF5Szq2DHwhhymvDoCn3RDdxRiZWaznHNRz6aMpbhf\nClwQUtyHOed+HNRmkdcmuLgPc85tD1nWGGAMQElJydBx48a17rfyVFZWUlzc8tEKmSRbsipn/PbX\nOWZuruP03rmYGfO31rJrn+PZz/bRpSCH/BzYtDf2D7G9Ohqb9jqO7JpDxT7H9hpHScdAsRrQNYcv\ndtZTU+vo1zmH/fWwvbqeI7rkcmhRoGhPWVdLr05Gn+Ic8qzp8fn76hz76qBboVHnXKD41dVSVJDP\nvvrA41yzxprUMGd9QwVz0HCGbKvAAAAGHUlEQVS/k0BhDhS33BxrUkTrI/y6jqbfOUWap2Ed9Q5q\na2vJz2u+E2AWuGBbbX3gUm85QH3DvN70aELLYCx/LRe0juDfaf+BQM7QbRjslJI8Tusd38UKR4wY\nEVNxD9xXMsIPcDrwdtDj24DbQtq8DZzuDecB2/DeOFr6GTp0qIvX5MmT45431bIlq3ImXrZkVc7E\nSnZOYKaLUredczEdLTMDGGhmR5pZAXAZMCGkzQTgam/4EuB9L4SIiKRB1M8FzrlaM7uRwN55LvCE\nc26Rmf2KwDvIBOBx4FkzWwbsIPAGICIiaRJTp49z7k3gzZBxdwUN1wCXJjaaiIjES2eoioj4kIq7\niIgPqbiLiPiQiruIiA+puIuI+FDUM1STtmKzrcDqOGfvQeBEqWyQLVmVM/GyJatyJlaycx7hnIt6\nK7C0Ffe2MLOZLpbTbzNAtmRVzsTLlqzKmViZklPdMiIiPqTiLiLiQ9la3MemO0ArZEtW5Uy8bMmq\nnImVETmzss9dREQiy9Y9dxERiSDrinu0m3WnIc8qM1tgZnPNbKY37hAzm2RmS73/u3vjzcwe9LLP\nN7MhSc72hJlt8e6U1TCu1dnM7Gqv/VIzuzrcupKQ8x4zW+9t17lmNipo2m1eziVmdkHQ+KQ+N8ys\nn5lNNrPFZrbIzH7qjc+obRohZ0ZtUzPrYGbTzWyel/Neb/yRZjbN2zbjvUuNY2aF3uNl3vT+0fKn\nIOtTZrYyaJue7I1P2+upUSwXfc+UHwKXHF4ODAAKgHnA4DRnWgX0CBn338Ct3vCtwP3e8CjgLQI3\nZzkNmJbkbMOBIcDCeLMBhwArvP+7e8PdU5DzHuDmMG0He3/3QuBI7/mQm4rnBtAbGOINdwa+8PJk\n1DaNkDOjtqm3XYq94Xxgmred/g5c5o1/BLjeG/4R8Ig3fBkwPlL+BP/tW8r6FHBJmPZpez01/GTb\nnnvjzbqdc/uBhpt1Z5qLgae94aeBbwWNf8YFfAp0M7PeyQrhnJtC4Pr6bcl2ATDJObfDObcTmASM\nTEHOllwMjHPO7XPOrQSWEXheJP254Zzb6Jyb7Q3vARYDfciwbRohZ0vSsk297VLpPcz3fhxwDvCS\nNz50ezZs55eAr5mZRcifMBGytiRtr6cG2Vbc+wBrgx6vI/KTNhUc8I6ZzbLAPWIBSpxzGyHwQgMO\n88ZnQv7WZktn5hu9j7RPNHR1RMiT0pxel8ApBPbgMnabhuSEDNumZpZrZnOBLQQK3XJgl3OuNsw6\nG/N403cDh6YiZ7iszrmGbfobb5v+0cwKQ7OGZErZ3z7binu4e82m+3CfM51zQ4ALgRvMbHiEtpmY\nv0FL2dKV+S/AUcDJwEbgD974tOc0s2LgZeAm51xFpKYtZEpJ1jA5M26bOufqnHMnA30J7G0fG2Gd\nad2eoVnN7HgC95QeBHyFQFfLLZmQFbKvuK8D+gU97gtsSFMWAJxzG7z/twCvEniCbm7obvH+3+I1\nz4T8rc2WlszOuc3ei6keeJSDH7PTmtPM8gkUzOedc694ozNum4bLmanb1Mu2Cygn0D/dzcwa7hIX\nvM7GPN70rgS681L6HA3KOtLrAnPOuX3Ak2TQNs224h7LzbpTxsw6mVnnhmHgfGAhTW8YfjXwmjc8\nAbjK+yb9NGB3w8f5FGpttreB882su/cx/nxvXFKFfBfxbQLbtSHnZd6RE0cCA4HppOC54fXvPg4s\nds49EDQpo7ZpSzkzbZuaWU8z6+YNFwHnEvh+YDJwidcsdHs2bOdLgPdd4FvKlvInTAtZPw96UzcC\n3w0Eb9P0vp6S8S1tMn8IfAv9BYG+uTvSnGUAgW/p5wGLGvIQ6Ad8D1jq/X+IO/iN+8Ne9gVAaZLz\nvUDg4/cBAnsM18aTDfgBgS+plgGjU5TzWS/HfAIvlN5B7e/wci4BLkzVcwM4i8BH6PnAXO9nVKZt\n0wg5M2qbAicCc7w8C4G7gl5X071t8yJQ6I3v4D1e5k0fEC1/CrK+723ThcBzHDyiJm2vp4YfnaEq\nIuJD2dYtIyIiMVBxFxHxIRV3EREfUnEXEfEhFXcRER9ScRcR8SEVdxERH1JxFxHxof8HJebgwuDq\n9XgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117855f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(LOSSES)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20180711_23h09'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now().strftime('%Y%m%d_%Hh%M')\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type FishNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(mynet.state_dict,'FishNet_'+now+'.pth')\n",
    "#torch.save(mynet,'FishNet_'+now+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FishNet_20180705_21h41.pth',\n",
       " 'FishNet_20180705_22h31.pth',\n",
       " 'FishNet_20180706_07h25.pth',\n",
       " 'FishNet_20180706_07h36.pth',\n",
       " 'FishNet_20180709_08h16.pth',\n",
       " 'FishNet_20180711_14h03.pth',\n",
       " 'FishNet_20180711_14h10.pth',\n",
       " 'FishNet_20180711_14h18.pth',\n",
       " 'FishNet_20180711_15h52.pth',\n",
       " 'FishNet_20180711_18h04.pth']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_path = glob.glob('*.pth')\n",
    "models_path.sort()\n",
    "models_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FishNet_20180711_18h04.pth'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_model = models_path[-1]\n",
    "mynet.load_state_dict(torch.load(latest_model)())\n",
    "#mynet.state_dict()\n",
    "latest_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.33% accuracy on the test set\n"
     ]
    }
   ],
   "source": [
    "errors = 0\n",
    "mynet.eval()\n",
    "\n",
    "for i, data in enumerate(testset):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "    outputs = mynet(inputs)\n",
    "    _,y = torch.max(outputs,1)\n",
    "    errors += (y!=labels).sum().item()\n",
    "    \n",
    "accuracy = 1-errors/len(test_ids)  # Previously len(testset which was wrong because it gave the number of batches)\n",
    "print('{:0.2f}% accuracy on the test set'.format(100*accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No clown fish\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAf8ElEQVR4nFV4WXMkyZGee0TkXVn3\ngULhbKAb6HPuGc5QK5JLcrmrlS1NXK0kk15ketG7zPRLZDLbJ73tm2zXjCNxuSRF7vAYDpvTnJ7p\nGXSjG91o3EAV6q68M8L1kFWFJswKVpYV6eHh/vnnnweKv/pHQAIEAkREAAQARKDsg0DZM0IAREQC\nBCQAIMRs7ezDIFuZGUAAhsQQsmVEkNmF2XfA+b/p82wHmBkHYLNnHFEHAgCOwAAFkMheJBBEhEBE\nM0cQAJGIgLIFmesAOH1AAECACEAwdS7bb+rW9FciRFIEDJEyb65cJ5geKfMWp/sAIWSLkZCIAFXm\nEoEESgEQgREwII5kEKYAgkAAKZpaVMA4ABBQZhcBgJCycEydzpzBWdCACACz9ezKuewXAkQFgAgK\nIEvF/ACzI02XTjcDAmAAiggAkGjmBCFmhiUSIXDCFEgBaAACSIFMIR5T4kO+BYzjNKJT4zh3lXAK\nH5w7SggAUhJJEAbM08Jm7xIR50BApADZFKHZma+Qk4UHpjGT0+NgFsjZczXFJgGgBJIAgJAACAIA\nxpGbxHQEAFI034RY9gISzjeap2bqRJYFQlAKgEApYJwoKycgGZOmA2NANA/s1NTU2szeHEjZm4iE\nV7h6NUcIRK+8LkARAJEw5xCdAzYDHhACMEAkUBme5sDPjkcAgAwzO1nm1bQUiGkoFUhJjM8CQABA\nDEFRBlpEJKCrY10lJ0MQzjBAczKZZQYAUIBSCMBQYuKhsFKmXf1IKQADJjLzQIoUAeNZimYHwZlF\nRBkB14AhEIGKgWsIDGQCfgdzVdLM+aGBpuVEWQUSUVZCmWeYZYUYpYQcGZtTFl7lLIsFCkxjkKHS\nHAYaUymAyFxCIkAxw74imuVMKcSZEwg4BRISESiFkKpsF6mQJDAAxoAZIvRSpsGMH6d/GXNMo0M4\nJ6kMswgUJ6gRgTZL0bwQ5xyJAkhx/0I5S8SNBAAUIaoZiFSWQgKakkzGf6QYKGKcGIc5OAGIaQQA\nSgIARCFTIZhFKTQ08ykRxCEiA6EB4NwLBAI2Ay9l4UBCzOqbhAEApNSsPrKVs0QAAJAAAuUsEQqg\naaEDzUA07QAK1XSbDKYERAQACgAA2YxIsyZEqBQhQ6FJ0BAI0wCYDogAjJQCSZmHMG0ds8aVbZvG\noBlzcE6b55Q75lU/LwYkBAHArjiYZiikV9pnBjiVFbSEjJmYBgSoCFBi1pKmdjIbipAhESkl0kAJ\nUkQoDEKGShICMgSaMTUSkAIZIyIxgUSgFJMJacYrxDfL9PQzywlmjWwWcCCEWS//owZJCtIQUZLI\ngZIwOUGjREbpSgIQIKqsdeE0gQwUAUKKFhCAlIgpMU5SAmPEcBp7xqaEG4dcJWSVphSGnKQkhjgD\nzFXZzeonO5wAUtNmRbPWkbXcGehmDV8QaqAUAAORt+JOxHXJTUA2ZTYEBEIuKnl9MArSzI3IA2ES\nCGAiQ/K0iNWsQ6lZP+E641pChEoCIsUhCB2AzyAwLxGaUeT03xT6RFleZjQ1UxNZ5Wd0kxFNplcS\ndBhJKQlw3tEUlxGYuVurha/2VXcUAwFxEwBBqWkDp2moCOctHQmIUaqQJYCkFCgJTMsgCmreVbNI\n0h+dB2DKQlMRl0kawFeR/0rO5iRHwPUETCAOoLjgSilSWSUKSJOHXx2GKQeFQAqAgDgiCOmroEvC\nJL1MXEeGRDMihoz6XgWtmros52Ce1fysRmcuKgFTS1MZQ0Rzj6diRaWgEshiCQoIpvVKCohUQtM+\nRoliGqRqHMbEGHANSKJ/inqO9FIKOgfN9M/yNh9TYSIFXvk1E7NESHJKR3GAuklMzIt1qjhfcS47\nvwBQWU/MbF3FPxOGAABgJH3g9Yj4K3Q8NZCRNADxsA3cSLUSMAaAqCJAQXpZpyCRqUKWGhWlF9Iw\n1vUQpUnIphJ1SmJzcCiUioQBRKDklEeVQpXpkblOzGALAoAwAw9JTDyFGgpr2vHUlI0TraRNQz57\nby7lpycBqZVMGkPqpfEQzBqoEIEDt6JsFCFFAAp5BHYUIyAByquQZsYBCJGphJhAxgjwqjUBECFI\niSApC9CsEATDRAEHRFQKhI6oZSw705oIgApElF49uspfJj9BAXACEWABiTgKJoOU2QCEMmRcl4q9\nUng45bkpo+MU3EoCETBGTFBW99laeqXvkgRSKIkIgBvZc2FhP0pQijwJDYBl0YI0BqZlY+TM64ym\n2Hxw0ZlcLeLIj7vjWDKbMBuGmNJKihRN1bCGkoBSwKnARlCAfDonQSabXxHHMgbUAfhUI80KfR5K\nAEYqBZnMBg8hYt21/X0YPgnLN8mupIQkFUdFJBXNJuJZDwQggCz1oEiBP1iiiS74aThJtSoyPpvv\nOJGa9R+az0ZERHFPYALmgpyTBc0m1OzFK86d5VldFQkBYRoxQEoCJXRAJvKublbvWd0Dm6crW/l8\ngYMwPc9/8nJweNqNSEhmA/ArJKqUgBGwVMFeUMgDy8f9UtIfUgx2LQljENbcMQCYxV5Rph1EAWis\nyYkEA0DQbPwHyCo1Ja4B4/PJIStOQH51BqYTKYUaZFXxw5/9VlA0DsPjU29yGRjjk6YT6I5xMQg+\nP6F/fnIauxuN1oaXwPNzjxLJYo+AE+hEhJSRMDFKGQTELKUEzOTllUzFOUSmsGAqJALiBqoUuKEJ\nhUAqTVLFiRvA2DzgAASkABiweSm8+kdi0O7dvVG6pmnvXnd6Z+1Rv3T50vv0o/tGPnfr2vaqm7e0\n/s03wsSpPjzLffJCfvyoE0YKJKCUoFQWZ4WMwAECAiUYSBkjcUA2HU+yOZBUNmAgEKEOCEgSlA+Q\nJqBp0YUhUpI5iVWiWceezjYMALLJEWbCc17a+I2/+R8MOos4apWNjbW6Y4mxHyk0Tg+PEyWNNNm6\nvZTP68CZMK3Eaf7zMznyqRNaP/vdUehHmHiCGSmxbIJHBI1REvSAELlJ3L5SqfPrgSndZ7cWvqYG\nidYAkjbzgBk+5Wh2kUHIARB1AQgUS5jP1FfsDcLf+z0gPUjiTzivfLpvK49FHheccR7a1eWNrdGR\ns9owKnpYKhmuGf/rDW65hb12dH5ufLWfhBK5gDRR0xavKFFAWgEJkBSQnBcx/hGfzK+ROBFCOgHu\n+lCaaUjK7pQywqCUMtKYi2PMpreMJdbf/o8oQYJA04REhtHYCbvW+IRUjIzrmonCtAXPG/DenXVh\n6YXG4rjf0w3TbCx/7pV/thu/PPcwiUo5dxzIJEmJ5vlFAVIBV8SmJANEpBDF1RUdSYjbTIXKWgWQ\nIBxCBqSQiIABInAGiqZtYer7NBMIAKhEIRrmGpsTMPw0UkYiivmQlsQpo96BlGmkAhWGQ5JDHW/1\n7FRSMhkL2x6OR89Oeo+pFSdlFIoQR5EvVUY6BASgUkBNAgFkkk4CpZAGIHKInKPkFNRcnUj0B1FK\nSQoRMsYwjbP2DwgoCaYaCVSCAAQMaH4vlX1R4ltfu6UlkX1t/Q9Pz87GYUhcSmIr26HXU3GAjGlc\nJGmoQ9LvjjTDeHp0GeWrYWHhk7bRlxxkHwAIeUoCSACyKfkwBJIELJvmhBylSQBKgnAtXTosnAy6\nE+ZWBRElw3CoUAenmaQpZkMVpEIOFHDUilICUQoAiBpcsULWOpRo1kvLKw3dzV9fcf1IdcdJAvzH\nP/p/nTBALhgpnXHSrMHEe9YZasVStHUnWVxOuFGoaKqnwpEPcZyEqUoCQAHCIGLTqwnk03JjlGAO\nmIlEHPwk1TopA33BT2CQAHIb3BYhQZIRczaRIpDuwLhcykXS6bRHxDUFHP9IGRAACR6ef/XpiwTE\n+/cWWRAPji72np74F6lTWbls75uca1KFKvFS6SRqQbe6w4CxSyZEo9lwLd7Pa6OBdJiJXjiZqETG\nDDFlLJsWgAAIQRIQIbJs8kyn91IKgAgYASAqQi2blKfNhVAxywM96niWjcQ0IkDyiZkzQTk7wCcf\nP+iP4te2G7tfDj5/3qu6mqP5vYsDZpZNhsPeiY/AmRC6ERrFaBL54wt/iNWiaVWL/f0L6QUlRqZu\nFYulgaOFoQ8KPCmEDIRhE0IsWeBRMA5ANxWxNM1krEIVIzIiBsgMOVIqSrQ6gZg5p4BAIYvBjj0F\nKCD1DSEjZcCMDRgkPL4Qv995qeviuNPWEInYQrN50r4Yel48GZMijQlO4GrWaqFac912r9fKW5Wy\n0bq2ZKSdhj486vbPoxwVHBx4cjgEhGDYRa/LBLebmwnqgrGSIbrjfn1lszuMwlgbTySqVAEnQCLJ\nGBJwTWAxz3oTSmVGurMJcVoShKhSxQHVrKUTASpeECnAaDKxNFHRxUrBxnTsR2FFZ3ZjI+bupHMM\nsV9zSl6q9O7ghpbrK78kT9cKzWLzJoQ7S5X8waU6SVzNMG3f98NRwrVRwihV/aMjI1cxHQtQaYnX\nefk8iRUhN8EElSaRSpmuCUtBPgauWN6QStdYquRs0IHZPJnp9hAhcOyWF08ZmUGKTIn3t5dHQVKv\nFhmlRydn+xedXpikSVw+3S/mGzqDCIFzPcc0lFE3DHzA85P+EZwavJ0MzxbK9mXbN+2I64XVWuP4\nPNBSqXSajAcpTkAwUiEjVnJ13TFCZZ93u9bkuaFb0mz4KSmWSPAp6bbqrbYPfhzNrwAFogQJqc9B\nMS2fcEOikVI6vV0AQEx0mggt9VUQP90fng4mAy+IpAQAHZkXJ7Lf1pApSikJHEtbtkqrC1XO0uc4\ngXwCbv5U5bhpV8wgCNup30VR+5Ovb1llnqTB7i/v7+wcDpGnms1U2h+PLJVYBSy7+tBDw2D1Mh+L\nWkeYw4lUhnk4EUoppvOpeANiSCRTQmAcNRMT4KQwStVslCJFlCgufvnkJCUAxhMlSRGf5Ywhk5Sa\nqDtMt6KoF51bZk7H1DYtbvle+9mb29UP3n3r+Yvzcy4NP7roDEuW0zBK50fHZkUvr7Zuc2vicV86\nyix1x6Nx2KmU607JdM1Korx2mibeS24Y9cqi0g1uOmMvlqkkxaKIZJimKRFqoJKYmbFC0DWIk7mk\nQyBAIe0K5kROcEEAikgqxQAZYqqkQAYADKBqOGvleqGY7/V63SAU5bqmwpohF1uFta3l+to1DeJY\nMWW1zk/owf2H/mBQbNaqm+VK1T3pTswgYqEfWI5ZLgCJ0VCGpJ+3+0nqN4rO7v65j3a+lLeLTiyw\nUMmHiTjve92+l3igIsCgi8GY7CoxAxSQnF5IMgQkkATCsnP1QnHojaum8MJ4FKQMmZ+EedOKkihU\nkuvsIvLjtCSqN6putVI2J/u/N3Ace+Pu0QlLY9/3O2MySx2nej1JE8konnjnz4I0qBbK+dj3Im9s\ncNioLJbq9ZfPno763cn4TDLNEtqNRcvOaaSSMI0SplE4kGNZCANiqSo7Co04bSRnsYi7XJQniSDT\nRGRJnCKQUL6SXHDdai5fWym2Kmqgh92Ddv9Fu+uSJoiqlpNznF4QxiCVDmtLWrOqqg1mv/evPvzf\nPwy8lOWZRtaLyyQOzMPdfYJjrlSpUOgPh7ovXuw9VzLRNJGzTNs2oygqlZyj/bPJ2O8PhiBpeLov\ndI4rC8yy6s1KY7HaHsj9UaQZvMCoVE50PWFuYdcst5+fmdGFJSx7qVkoWofH/cQjGvaZEsJmxvG5\nB5cvL7VgsWgWq40ttAuttYUiaZQSsSCJNtdbwXgkNF0zjVKt3gtAopESluv1N99/+857IvQS3a11\nOr3Ls04SYqFsNxfcTqf99Onzl0eXgjnVxmK5VX+6vz8cJ8FgHIQhEimpuOCNQC606m7OLbr55oLR\nqI45FyMvnIzGiEk87i/KQa2FnWennnIN39LLrFkilce4WBkNEBfsumVYbqGu7LL0esP+Uc4wi6Um\niNQU6AgulbJ1c2GhyfP50WTcbrdfHp5wKW0hNlvlmzcLTnmRM1VttjTdAgZI6ujpjlRUb1Z6F4eo\nl/KLd3ILK6s3t3/35dH/+bt/2N/7yjaVqenD3sjStEa1vHl9TTJuu26lUctXy24l//RgHxledifD\nXr9z1vfC1NRFya2RVvdNdxgNotA3GBsnSpig2sNOEnnV0jiWkKRpX3n94AUBcYSiYZiCeZHyvGQi\nZSTTN++tLZSE7/m2ZVkUl82Aq5OT/cu0f1KuVsJY3vr6n1um/uCj3/bjZ4POBenDcOIPT74IL3Za\nje3lAo4tPo7TVKNyo6TC1HFz49FEaWZEYhica+ejQqG8s/vMssAolPrdsFCusPHY0smpaWvrJWDO\nkxfBeS9cqOcbi2X8Wr65H/q+kvP7Ul3wbIJigDjjIiEYYyxv699+f+vOmzf6/VM7L0qGTEaTnaej\n3lBiFF2rMG67xsq9lZXisy+eOmzkT8bDSANuGaV10+C26w67w9OzbncSHvbGIQlK08VaUUpKdDGK\n6OSk65rundubiRxFo4FuWna5xk3he2NSWgp8fXP53Q/esS2jczmQQIiIf7q21Z5Mer4fK1JEmhCW\nrgVRCAgMp9dYDEnnsFjga017e7Vw64PXSxtlrWjYOd0wCmfHk/Zp73jnpP/4RbcXHPfCe9eLhpNT\nqYokVRrN4kKzUm8NL06KixvNjXvnZ6f7uzvAmVmo3f/s+fH5pUpjtyhipfe7nowYGtGtW4uLrSKA\nFsc8HA3MnK6ZWqD0o5NRnLLNzWvXVheKZUdoQryxke8EbnscnvQnl/2hQlSca4ZWKRZytpkGIQos\nGFR02Lt3K5ZGUUqcV5hscJokXqxxb2nNMI2yTvxFoOeboTjp53Ly+GL4cPdSkjLN09VWfanVQM2K\nvpy89TU8v3h5/PJFuZIfvHy2mXffvnvneMTjJL6+UfH96GC/fXB8NO4N+1w2lyuGAZhSoeIkUVIt\nWzpGo3Hy2aef/eJn0Zv3NhbqRfzvH6yOQhjEPNHcfqL2zzobK7Wtpt6diO0bN3udjuZY6zU/Z9sn\nnd445JqunNLC1r1VFbXtsrm8dc0by3/4258nEfVOO0IwJ5fTNX7pRw+fHJi27pqaI/hCLVdtNoq1\ntUS57dOLGILlpfqkd7yyfUsmSWn1Rm/IpUh0HuVcBwX/7A+PLSN0CxoKXabYWK75XtjvJw8fPFNh\nYrulB18eI+D19Rb+1zdXS8VCvllLgPU9nCi6veqYcqCM3PlZ/6IzqS62trZKUtJBt5NI/PZ3NzTT\nrDQbB0/OrJJRriz+4//6xd7OSUICGfoTT8Zhzs25eZsX8+/9i01OehJHe09PopCdn49GY37R7hi6\nckw0LVMvlJMUvYRt3Fi5dre1/drqydGFLqC5Uh8OvckkjKKwdzEwXbNYLR2+6J0edvud4XKrPh6P\nwmASR4loNMsTT9ZazZtbWvt0OBqr6uJ6EHia5rtmUK8IoaUX/eEHf/H2t7b/1LIcSkJvODD1dOv1\nlWF7uPOb318ctYuucXra0XTTduxREjuWQcBcSz/fOxWCX9tqfeO7b57uHTfWbnz1ZfukfRZMovM+\nvbnYGHnJYTD5k+/efvebq14wOjl6auomY6LfP2kt3wgn+PmDL9oXo/W8ZRni5ODi2eOTguvEYVKr\nVqPU8gd9kQZevrTY6yQXhcAfXVRK1W53vHTtlox2f/5P55HC197cblaqcVfXpa0C9fz++ahzuL5V\nPHh+9GznZO9g8OJwoCMvuPZZLwqDQc7Wz9udOzcLusTRMKeZmj/p1lpV24bb68Xu8OzLJ7rvhajp\nfUYLi3UeGsofmLrHhHZ+dMaqxbWttWGvD+AtLC1E0VatumTmMYXojbc2b93c1FC3LJNHwpvEX00e\n43/7zvvADErZB+/dXKiWikvG0cmTQmPZNrvPHr2UzN56+z1HdFUaxmlKTMSJNfH0vKOOd5/sH5zs\n7A2evhgVXb1cX/BHUWcwkqkiYu+81sxV61ESl10leKKZxvLK5uHLoeWI/a74yc8+TyW8s3GnWDYs\nVJ2g5zaMzbfrtXrJypm6Y6ehjAJ2edo7PRiMx2mh0kqB3b61TvHQdSu+NwAFmpnbPxuIoVEjGY3H\nvfu/etyy6ne+/16qCi+ePLnoRfe23UJenL7YXV0yGIRcJcLQVXrJpdILr9946+vF8i8dW1ZdWchZ\n1ZXFD3/dPu6fc2TVnINSC4NofWs9nZx99sV5syyWGpVqzT0/7L+2datWWfz49zuN1XzONEtWtf/w\ni49/s6/S3L4YQk5Wm6XDx+3x6aRgI8stLizrhjvWTXfYHfaOD/NFf9w9vTw/YJpWajRwKVchgHu1\nxbVCOU7Cb/7b263aOCTx4Bn/wX/4Lk6efPzRI6Hnbt9ABhFynelGHMahtBauv9Pe+VH3/IIIul1J\nzsLjg/SX9/dUlCLnC/XC6zfXb7+9WSkylCGmUbVRUpo7iYrBuGdo+YtOGPVOXV2NoXp6EqgQv/7W\n271u5+HL3W/9zQeCwv7OhX+ZjC+Hi++sffzoi37/cmWzIeOoexzm3VISJZQOUXnC5nqs5PHEa5Sa\nF2H/R7/54j99u8iF+Mvvvbt0fUPF7tpx/PkPH+5zR3HDm/QJWblc0lzXOn6kW7nF6wVIg1wVPFzx\nBo/+y1+/tr/biWO1d+6v3txcqgTt/V037wSpGtrG0HeO9r9aWimVF9n169Wdj04lRXfubtTLPhdV\nq6Zvrm3k15y1G0XFRpqsPHv6ooiaOUy/1traGcjR3nDI/Ek3OdzZEQJIY/V6EdfcWqLUmrNwvdp0\ni0mfd5cq9NbrjdXtZeKlcm3xVz/9+fFL/4vH+2eXvi9JEs+7zt11e72qdUdo1Y2N67UXx+rieGgm\nl+9/8zYo68WXLwvN65EaOXbcKEdpGDNNsysLR536Jz/+7O6NW7WVxCgYEDl+O9Kd1qMv9vKFytY7\n69vfXrFzth+2g9g7/v1F8DTIl+tmLqxca3310d6nn+3d338WpWHi9esFpnHGdSEiKRmyD7ZubuUN\nWmbVNYeRLBf54OQY8ZQmBxvrxQnUXvxqbzAhxhmRLFFUMiln6FZNxEh/uH+Yc5y3Xt8KveZoENx9\nb6tQcblmP92Nzl4ery21dFd6E58r6bj6tXvLzXXx5f0HjYaRr63vPvWr5cnNzcaTvcO9+20VX26+\nfVPUmM7z197X2uXhJ3/7yXa93Nq+tXBzrfvLB4xHwJQpck5Bk2lKqAlJtF6q5EV8OPD+6Q+Pv/En\nK9//3lI4bBsmcsQgUGOs/+GT31gMNVcHhgBqqcTXmma9YQvDNIoN8XR00NW6o/jO9TpiIpwmmjQ8\n+sP23Tf115dk0HWLdn9waumtSmH5lx89vr6qWk2zXMvZBf3m+82Cwy3DzS2upzEcPnrZPuiuvb9R\n31oEZNogMi4Hnx+eaqa2Gw/08vjPv3OttLAk9Dym8enBAJWGLbuS181bC/XdiwsJ9K23lu9ul2KJ\nuQIzDaMz0S5PTtx4aORyzAgvup5t8dWmVS5pqGu5cg6E+9nn45ByH7x33Ts/kXxcXHmLoXH89EG7\nx8sqLzTN84a3vvmBbY0J6O8/fDga9l/b0IpFzSlUT0f1nJ5yZpEKTp4ftJYX9h+PDk5GdbNAUeBo\nFg4C3WB3vvd2N+lNjDGzyZdg5XILq01NNyhFsVwueGF8OQmvlerXtx3biJ8+71x2R+NAcKGKReOd\nm/r160uVpWUzXwi8dhpMwsmYEBVgnIrHz+LPdn3GZTF/trjg6KZ7OdRtbg3HC2cPHot1u1Ktu+3o\n8Ye/qt21cjknJ+KffnpwY/lWM2dyLv1R+3Iwee2dbcfJ9y8NkcuJYkQd9aJzbgdKqnM3VxKmuL7I\nKrnFYjTyRv3B0UUoPN3UNUOubGyKVtUUzDAhb5jG9nYx3u89H9N3Xrt9/8s9KIvWcrlQjaQQSmN2\nrdW4szU532/vv4zj5LIb/fy3w59+fNQdDAnYrz9/2aiUlhdrjPa+8cE1jE5XahYABKPwpDvQTf75\nD59KLjWn4AB/8tgj1IeTzs4zD0Ectb/a2G4t1Ncks9P00tbV4uvNasPJY+qHUN9cFuKy/eIsDMN8\nIbe5VVWi/PFHjzQbK/ma+Pq1W8u3XLssB+30J796tNaqOzw62+0c94cvng/Vg5cri7U3Xl+5M/YX\nhzurtGEYRW7Vdp/1fvPxxa8/fd7zglQRIIxHQ4crL/Qt2ykWlKT+eK21fm2tdzzWjrBcKu21T7Si\n3mgtlOo1y9FBsXGvf/j4IFIUkrr/O7NWLZbyOVujv/jBtyzLybu+f7nb744e/vb53TcaulCoxaP2\nqeeHuVL+3u3qZw97v/i7n4hmpVnxix3/+RvfW1lq1Uult8PjU3dj6fvF/I8//OGTT3e9KPjww9/9\nulhsNQqta0PNsYX0osuTdJzoHCuuTUCKWKlQWC4Xb9fLiaZb7sbZxWB3b2IZZ8WcW2o4tZv1f/e9\nfyNYYltWGktNQ2Hw3mh9tdZ4+ujg87N2EsW28nRgf/ZX39BS73LvpbZS0Mx8Pu/H3mW+vJkrl61c\nYXR50e9Mnn15mBzt3r2+OBy6+NdLKzcWbq4x2Xiz2bTXQ9MscIyDiLcauzuPc5EiEf79l79ipq67\nxQc7R6PRxHVEa6Fyo7V0dnFGwlpZXVperot4PL44i0KfmCg36hv33kwir1Kxc+UqEKahJycjhkkS\nJZNJYJm6W8oRZ2SYqaJOt//Fo8PjnaMbt1q37617ZweUBMI0pJkrNYv7j4/yxYXatYLQKqaeDtoX\n4SgmFEqGTLfwXqHkGoUfbF2/tbReTOjnjx7q17YXKuXnxwed9unr1cWtf3nvf/7s/3Y9daNYI783\nijtSxqsb26997bY/8lVC8fji8uzAMLC6vDboj472D5WCUnP9L//zv9eVr8YTGU4YpmkcySRO05iI\nGZaVK7oyThIpOadEkbO4/NVnZ0fPdrdvl5uNwqTfA80YDpJ8qSBsF8DyvcB0nGK9mKST0OslPtcM\ndzAK/z/avrpOsW8KlwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x11781D0F0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(RESULT[y[0].item()])\n",
    "transforms.ToPILImage()(inputs[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
